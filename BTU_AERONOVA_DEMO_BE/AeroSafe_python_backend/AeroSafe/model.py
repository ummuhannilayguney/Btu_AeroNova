# aqi_forecast.py (airnow_viewer entegreli)
# -----------------------------------------------------------------------------
# Turkish quickstart:
# 1) pip install pandas numpy scikit-learn joblib matplotlib
# 2) python model.py --data_csv path/to/data.csv --city "Denver, CO" --parameter PM25 --horizon 24 --mode train_and_forecast
#
# YENƒ∞ √ñZELLƒ∞KLER (airnow_viewer entegrasyonu):
# - ≈ûehir listesi: python model.py --list-cities
# - ≈ûehir √∂nerileri: python model.py --suggest-city "Den"
# - Otomatik koordinat √ßekme: --auto-coords (varsayƒ±lan olarak aktif)
# - Detaylƒ± √ßƒ±ktƒ±: --verbose
#
# This script:
# - airnow_viewer.py'den ≈üehir listesi ve koordinatlarƒ± otomatik √ßeker
# - Loads your existing CSV with AirNow-like columns (ReportingArea, StateCode, ParameterName, AQI, DateObserved, HourObserved, optional temp_c, humidity)
# - Cleans & normalizes data (build hourly ts, dedup, small-gap ffill, winsorize)
# - Feature engineering: lags, rolling stats, calendar, optional exogenous (temp/humidity)
# - Trains GradientBoostingRegressor (fallback RF), backtests vs baselines, produces forecasts
# - Adds conformal prediction intervals from validation residuals
# - Saves artifacts under out_dir/<City>/<Parameter>/
# -----------------------------------------------------------------------------

from __future__ import annotations
import argparse
import json
import os
import sys
import warnings
import importlib.util
from dataclasses import dataclass
from typing import Tuple, List, Dict, Optional
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import threading
import time
import requests
from requests.adapters import HTTPAdapter

import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.utils import check_random_state
from sklearn.exceptions import NotFittedError
import joblib
from math import sqrt
from datetime import datetime, timedelta

warnings.filterwarnings("ignore", category=FutureWarning)

# --------------------------- Production Configuration -----------------------
# Configuration constants
AQI_PARAM_MAP = {
    'PM2.5': 'PM25',
    'PM25': 'PM25', 
    'OZONE': 'O3',
    'O3': 'O3'
}

# Production-grade configuration
# Daily horizons 1..7 days (in hours)
HORIZONS = [24, 48, 72, 96, 120, 144, 168]
N_FOLDS = 5
INTERVAL_METHOD = "quantile"  # "quantile" | "conformal" | "none"
USE_EXOG = True
MIN_TRAIN_DAYS = max(60, 90)  # prefer 90‚Äì120 days for 168h horizon
STEP_HOURS = 24
RANDOM_STATE = 42

# Enhanced configuration for production
AQI_THRESHOLDS = {
    'Good': (0, 50),
    'Moderate': (51, 100), 
    'Unhealthy for Sensitive': (101, 150),
    'Unhealthy': (151, 200),
    'Very Unhealthy': (201, 300),
    'Hazardous': (301, 500)
}

# Training thresholds (horizon-aware) - adjusted for practical usage
MIN_SAMPLES_PER_HORIZON = {
    24:  75,
    48:  100,
    72:  125,
    96:  160,
    120: 190,
    144: 220,
    168: 260,
}
COVERAGE_THRESHOLD = 0.7
WINSORIZE_LIMITS = (0.01, 0.99)

# COVERAGE-CONTROL: Coverage adjustment parameters
COVERAGE_TARGET = 0.8
COVERAGE_TOLERANCE = 0.05
COVERAGE_ADJUSTMENT_FACTOR = 0.1

# BLEND-LOCAL: Ensemble blending parameters  
BLEND_LOCAL_WEIGHT = 0.2
BLEND_GLOBAL_WEIGHT = 0.8
MIN_LOCAL_SAMPLES = 50

# Feature availability detection
try:
    from sklearn.ensemble import HistGradientBoostingRegressor
    HAS_QUANTILE_GBDT = True
except ImportError:
    HAS_QUANTILE_GBDT = False

# Optional dependencies with graceful fallbacks
try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
    print("‚úì LightGBM available - using for enhanced performance")
except ImportError:
    HAS_LIGHTGBM = False
    lgb = None
    print("WARNING: LightGBM not available - falling back to scikit-learn")

try:
    import catboost as cb
    HAS_CATBOOST = True
    print("‚úì CatBoost available")
except ImportError:
    HAS_CATBOOST = False
    cb = None


# --------------------------- AirNow Viewer Parameters Import ----------------

def import_airnow_params(airnow_viewer_path: str = "airnow_viewer.py") -> Dict:
    """
    airnow_viewer.py dosyasƒ±ndan parametreleri otomatik olarak √ßeker.
    
    Args:
        airnow_viewer_path (str): airnow_viewer.py dosyasƒ±nƒ±n yolu
        
    Returns:
        Dict: airnow_viewer parametrelerini i√ßeren s√∂zl√ºk
    """
    try:
        # airnow_viewer.py dosyasƒ±nƒ±n tam yolunu belirle
        if not os.path.isabs(airnow_viewer_path):
            airnow_viewer_path = os.path.join(os.path.dirname(__file__), airnow_viewer_path)
        
        if not os.path.exists(airnow_viewer_path):
            print(f"Warning: {airnow_viewer_path} dosyasƒ± bulunamadƒ±. Varsayƒ±lan parametreler kullanƒ±lacak.")
            return {}
            
        # Mod√ºl√º dinamik olarak import et
        spec = importlib.util.spec_from_file_location("airnow_viewer", airnow_viewer_path)
        airnow_module = importlib.util.module_from_spec(spec)
        
        # Ge√ßici olarak sys.modules'e ekle (import hatalarƒ±nƒ± √∂nlemek i√ßin)
        old_modules = sys.modules.copy()
        try:
            spec.loader.exec_module(airnow_module)
        except Exception as e:
            # Import ba≈üarƒ±sƒ±z olursa varsayƒ±lan parametreleri kullan
            print(f"Warning: airnow_viewer.py import edilemedi: {e}")
            return {}
        finally:
            # sys.modules'√º temizle
            for key in list(sys.modules.keys()):
                if key not in old_modules:
                    del sys.modules[key]
        
        # AirNowViewer sƒ±nƒ±fƒ±ndan parametreleri √ßek
        if hasattr(airnow_module, 'AirNowViewer'):
            # GUI olu≈üturmadan sadece parametreleri al
            # cities dict'ini direkt tanƒ±mla (AirNowViewer.__init__'tan)
            cities_dict = {
                "Albany, NY": (42.6526, -73.7562),
                "Albuquerque, NM": (35.0844, -106.6504),
                "Anchorage, AK": (61.2181, -149.9003),
                "Atlanta, GA": (33.7490, -84.3880),
                "Austin, TX": (30.2672, -97.7431),
                "Bakersfield, CA": (35.3733, -119.0187),
                "Baltimore, MD": (39.2904, -76.6122),
                "Birmingham, AL": (33.5186, -86.8104),
                "Boise, ID": (43.6150, -116.2023),
                "Boston, MA": (42.3601, -71.0589),
                "Buffalo, NY": (42.8864, -78.8784),
                "Charleston, SC": (32.7765, -79.9311),
                "Charlotte, NC": (35.2271, -80.8431),
                "Chicago, IL": (41.8781, -87.6298),
                "Cincinnati, OH": (39.1031, -84.5120),
                "Cleveland, OH": (41.4993, -81.6944),
                "Colorado Springs, CO": (38.8339, -104.8214),
                "Columbus, OH": (39.9612, -82.9988),
                "Dallas, TX": (32.7767, -96.7970),
                "Denver, CO": (39.7392, -104.9903),
                "Des Moines, IA": (41.5868, -93.6250),
                "Detroit, MI": (42.3314, -83.0458),
                "Fort Worth, TX": (32.7555, -97.3308),
                "Fresno, CA": (36.7378, -119.7871),
                "Hartford, CT": (41.7658, -72.6734),
                "Honolulu, HI": (21.3099, -157.8581),
                "Houston, TX": (29.7604, -95.3698),
                "Indianapolis, IN": (39.7684, -86.1581),
                "Jacksonville, FL": (30.3322, -81.6557),
                "Kansas City, MO": (39.0997, -94.5786),
                "Las Vegas, NV": (36.1699, -115.1398),
                "Little Rock, AR": (34.7465, -92.2896),
                "Long Beach, CA": (33.7701, -118.1937),
                "Los Angeles, CA": (34.0522, -118.2437),
                "Louisville, KY": (38.2527, -85.7585),
                "Madison, WI": (43.0731, -89.4012),
                "Memphis, TN": (35.1495, -90.0490),
                "Mesa, AZ": (33.4152, -111.8315),
                "Miami Beach, FL": (25.7907, -80.1300),
                "Miami, FL": (25.7617, -80.1918),
                "Milwaukee, WI": (43.0389, -87.9065),
                "Minneapolis, MN": (44.9778, -93.2650),
                "Nashville, TN": (36.1627, -86.7816),
                "New Haven, CT": (41.3083, -72.9279),
                "New Orleans, LA": (29.9511, -90.0715),
                "New York, NY": (40.7128, -74.0060),
                "Norfolk, VA": (36.8468, -76.2852),
                "Oklahoma City, OK": (35.4676, -97.5164),
                "Omaha, NE": (41.2565, -95.9345),
                "Orlando, FL": (28.5383, -81.3792),
                "Philadelphia, PA": (39.9526, -75.1652),
                "Phoenix, AZ": (33.4484, -112.0740),
                "Pittsburgh, PA": (40.4406, -79.9959),
                "Portland, OR": (45.5152, -122.6784),
                "Providence, RI": (41.8240, -71.4128),
                "Raleigh, NC": (35.7796, -78.6382),
                "Richmond, VA": (37.5407, -77.4360),
                "Rochester, NY": (43.1566, -77.6088),
                "Sacramento, CA": (38.5816, -121.4944),
                "Salt Lake City, UT": (40.7608, -111.8910),
                "San Antonio, TX": (29.4241, -98.4936),
                "San Diego, CA": (32.7157, -117.1611),
                "San Francisco, CA": (37.7749, -122.4194),
                "San Jose, CA": (37.3382, -121.8863),
                "Seattle, WA": (47.6062, -122.3321),
                "Shreveport, LA": (32.5252, -93.7502),
                "Spokane, WA": (47.6587, -117.4260),
                "St. Louis, MO": (38.6270, -90.1994),
                "St. Paul, MN": (44.9537, -93.0900),
                "Tampa, FL": (27.9506, -82.4572),
                "Tucson, AZ": (32.2226, -110.9747),
                "Virginia Beach, VA": (36.8529, -75.9780),
                "Washington, DC": (38.9072, -77.0369)
            }
            
            params = {
                'cities': cities_dict,
                'api_key': "F61C7D0A-FFD3-41DB-9FEF-895F39E29ECC",
                'base_url': "https://www.airnowapi.org/aq/observation/latLong/current/",
                'available_cities': list(cities_dict.keys()),
                'default_parameters': ['PM25', 'O3']
            }
            
            return params
        else:
            print("Warning: AirNowViewer sƒ±nƒ±fƒ± bulunamadƒ±.")
            return {}
            
    except Exception as e:
        print(f"Error: airnow_viewer.py parametreleri y√ºklenemedi: {e}")
        return {}

def get_city_coordinates(city_name: str, airnow_params: Dict) -> Tuple[float, float]:
    """
    ≈ûehir adƒ±ndan koordinatlarƒ± √ßeker.
    
    Args:
        city_name (str): ≈ûehir adƒ±
        airnow_params (Dict): airnow_viewer parametreleri
        
    Returns:
        Tuple[float, float]: (latitude, longitude)
    """
    if 'cities' not in airnow_params:
        raise ValueError("airnow_viewer parametreleri y√ºklenemedi.")
    
    cities = airnow_params['cities']
    
    # Tam e≈üle≈üme kontrol√º
    if city_name in cities:
        return cities[city_name]
    
    # B√ºy√ºk/k√º√ß√ºk harf duyarsƒ±z arama
    city_lower = city_name.lower()
    for city, coords in cities.items():
        if city.lower() == city_lower:
            return coords
    
    # Kƒ±smi e≈üle≈üme (≈üehir adƒ±nƒ±n ba≈ülangƒ±cƒ±)
    for city, coords in cities.items():
        if city.lower().startswith(city_lower.split(',')[0].strip().lower()):
            print(f"Info: '{city_name}' i√ßin kƒ±smi e≈üle≈üme bulundu: '{city}'")
            return coords
    
    # Hi√ß e≈üle≈üme bulunamadƒ±
    available_cities = list(cities.keys())[:10]  # ƒ∞lk 10 ≈üehri g√∂ster
    raise ValueError(f"≈ûehir '{city_name}' bulunamadƒ±. Mevcut ≈üehirlerden √∂rnekler: {available_cities}")

class RealTimeDataFetcher:
    """AirNow API'den ger√ßek zamanlƒ± veri √ßeken sƒ±nƒ±f"""
    
    def __init__(self, api_key: str = "F61C7D0A-FFD3-41DB-9FEF-895F39E29ECC", airnow_params: Dict = None):
        self.api_key = api_key
        self.base_url = "https://www.airnowapi.org/aq/observation/latLong/current/"
        self.session = requests.Session()
        self.airnow_params = airnow_params or {}
        
        # Retry stratejisi ekle
        retry_strategy = requests.adapters.Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def fetch_current_aqi(self, lat: float, lon: float, distance: float = 25) -> List[Dict]:
        """
        Belirtilen koordinatlardan ger√ßek zamanlƒ± AQI verisi √ßeker
        
        Args:
            lat (float): Enlem
            lon (float): Boylam  
            distance (float): Arama mesafesi (mil)
            
        Returns:
            List[Dict]: AQI verileri listesi
        """
        try:
            params = {
                'format': 'application/json',
                'latitude': lat,
                'longitude': lon,
                'distance': distance,
                'API_KEY': self.api_key
            }
            
            # Kƒ±sa timeout ile deneme
            response = self.session.get(self.base_url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            return data if data else []
            
        except requests.exceptions.RequestException as e:
            print(f"‚ùå API veri √ßekme hatasƒ±: {e}")
            print("‚ö†Ô∏è API eri≈üilemediƒüi i√ßin sentetik veri kullanƒ±lacak")
            return []
        except Exception as e:
            print(f"‚ùå Genel hata: {e}")
            return []
            return []
    
    def create_historical_data(self, lat: float, lon: float, city: str, days: int = 30) -> pd.DataFrame:
        """
        Ger√ßek API verisi ile ge√ßmi≈ü veri sim√ºlasyonu olu≈üturur
        """
        try:
            # Mevcut veriyi √ßek
            current_data = self.fetch_current_aqi(lat, lon)
            
            if not current_data:
                print("‚ùå Mevcut veri √ßekilemedi, sentetik veri olu≈üturuluyor...")
                return self.create_synthetic_data(city, days)
            
            # Ger√ßek veriden parametreleri al ve normalize et
            parameters = []
            for item in current_data:
                param = item.get('ParameterName', 'PM25')
                # API'den gelen parametreleri normalize et
                if param == 'PM2.5':
                    param = 'PM25'
                elif param == 'PM10':
                    param = 'PM10'  # PM10'u destekleyelim
                elif param == 'O3':
                    param = 'O3'
                parameters.append(param)
            
            parameters = list(set(parameters))  # Unique hale getir
            
            # Ge√ßmi≈ü veri sim√ºlasyonu olu≈ütur
            records = []
            base_time = datetime.now() - timedelta(days=days)
            
            for param in parameters:
                # Mevcut deƒüeri referans al
                current_value = None
                for item in current_data:
                    if item.get('ParameterName') == param:
                        current_value = item.get('AQI', 50)
                        break
                
                if current_value is None:
                    current_value = 50  # Varsayƒ±lan
                
                # Ge√ßmi≈ü veri olu≈ütur
                for i in range(days * 24):  # Saatlik veri
                    timestamp = base_time + timedelta(hours=i)
                    
                    # Mevcut deƒüer etrafƒ±nda realistik varyasyon
                    variation = np.random.normal(0, current_value * 0.2)
                    aqi_value = max(0, current_value + variation)
                    
                    # G√ºnl√ºk ve haftalƒ±k pattern ekle
                    hour_effect = 10 * np.sin(2 * np.pi * timestamp.hour / 24)
                    day_effect = 5 * np.sin(2 * np.pi * timestamp.weekday() / 7)
                    
                    aqi_value += hour_effect + day_effect
                    aqi_value = max(0, min(500, aqi_value))  # AQI sƒ±nƒ±rlarƒ±
                    
                    city_key = f"{city.split(',')[0]}, {city.split(',')[-1].strip() if ',' in city else 'XX'}"
                    records.append({
                        'DateObserved': timestamp.strftime('%Y-%m-%d'),
                        'HourObserved': timestamp.hour,
                        'ReportingArea': city.split(',')[0],
                        'StateCode': city.split(',')[-1].strip() if ',' in city else 'XX',
                        'ParameterName': param,  # Normalize edilmi≈ü param kullan
                        'AQI': int(aqi_value),
                        'Latitude': lat,
                        'Longitude': lon,
                        'CityKey': city_key,
                        'CityKeyNorm': safe_city_key(city_key)
                    })
            
            df = pd.DataFrame(records)
            
            # ts s√ºtununu olu≈ütur (DateObserved + HourObserved'dan)
            if not df.empty:
                df['ts'] = pd.to_datetime(df['DateObserved'] + ' ' + df['HourObserved'].astype(str) + ':00:00')
            
            print(f"‚úÖ {city} i√ßin {len(df)} saatlik ger√ßek API tabanlƒ± veri olu≈üturuldu")
            return df
            
        except Exception as e:
            print(f"‚ùå Ge√ßmi≈ü veri olu≈üturma hatasƒ±: {e}")
            return self.create_synthetic_data(city, days)
    
    def create_synthetic_data(self, city: str, days: int = 30) -> pd.DataFrame:
        """Fallback: Sentetik veri olu≈ütur - API eri≈üilemediƒüinde kullanƒ±lƒ±r"""
        print(f"üìä {city} i√ßin sentetik veri olu≈üturuluyor... (API eri≈üilemediƒüi i√ßin)")
        
        records = []
        base_time = datetime.now() - timedelta(days=days)
        
        parameters = ['PM25', 'O3']
        
        # ≈ûehir koordinatlarƒ±nƒ± al
        if city in self.airnow_params.get('cities', {}):
            lat, lon = self.airnow_params['cities'][city]
        else:
            lat, lon = 39.7392, -104.9903  # Denver varsayƒ±lan
        
        for param in parameters:
            # Parametre bazlƒ± ger√ßek√ßi deƒüerler
            if param == 'PM25':
                base_value = 50  # PM2.5 i√ßin normal deƒüer
                variation_range = 30
            else:  # O3
                base_value = 35  # Ozon i√ßin normal deƒüer  
                variation_range = 20
            
            for i in range(days * 24):
                timestamp = base_time + timedelta(hours=i)
                
                # Realistik AQI deƒüeri - √ßoklu pattern
                seasonal = 15 * np.sin(2 * np.pi * i / (24 * 7))  # Haftalƒ±k pattern
                daily = 20 * np.sin(2 * np.pi * timestamp.hour / 24 - np.pi/4)  # G√ºnl√ºk pattern (√∂ƒüleden sonra pik)
                weather_effect = 10 * np.sin(2 * np.pi * i / (24 * 3))  # 3 g√ºnl√ºk hava durumu
                random_noise = np.random.normal(0, 5)
                
                aqi_value = base_value + seasonal + daily + weather_effect + random_noise
                aqi_value = max(0, min(300, aqi_value))  # AQI sƒ±nƒ±rlarƒ±
                
                city_key = f"{city.split(',')[0]}, {city.split(',')[-1].strip() if ',' in city else 'XX'}"
                records.append({
                    'DateObserved': timestamp.strftime('%Y-%m-%d'),
                    'HourObserved': timestamp.hour,
                    'ReportingArea': city.split(',')[0],
                    'StateCode': city.split(',')[-1].strip() if ',' in city else 'XX',
                    'ParameterName': param,
                    'AQI': int(aqi_value),
                    'Latitude': lat,  # Ger√ßek ≈üehir koordinatlarƒ±
                    'Longitude': lon,
                    'CityKey': city_key,
                    'CityKeyNorm': safe_city_key(city_key)
                })
        
        df = pd.DataFrame(records)
        
        # ts s√ºtununu olu≈ütur (DateObserved + HourObserved'dan)
        if not df.empty:
            df['ts'] = pd.to_datetime(df['DateObserved'] + ' ' + df['HourObserved'].astype(str) + ':00:00')
        
        return df

class AQIForecastGUI:
    """AQI tahmin sonu√ßlarƒ±nƒ± g√∂steren Tkinter GUI"""
    
    def __init__(self, airnow_params: Dict):
        self.airnow_params = airnow_params
        self.root = tk.Tk()
        self.root.title("üåç AeroSafe - AQI Tahmin Sistemi")
        self.root.geometry("1200x800")  # Larger for advanced panel
        
        # GUI-TIER: Initialize advanced panel state
        self.advanced_visible = False
        self.last_forecast_cache = None
        self.last_coverage_metrics = None
        self.drift_metrics_cache = None
        
        # Ana frame
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Configure grid weights for resizing
        self.root.columnconfigure(0, weight=1)
        self.root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=1)
        main_frame.rowconfigure(8, weight=1)
        
        # Top bar with controls and advanced toggle
        top_frame = ttk.Frame(main_frame)
        top_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        top_frame.columnconfigure(3, weight=1)  # Make space for advanced button
        
        # GUI-TIER: Advanced panel toggle button (top-right)
        self.advanced_toggle_var = tk.StringVar(value="Show advanced details ‚ñ∏")
        self.advanced_toggle_btn = ttk.Button(top_frame, textvariable=self.advanced_toggle_var, 
                                            command=self.toggle_advanced_panel)
        self.advanced_toggle_btn.grid(row=0, column=4, sticky=tk.E, padx=(20, 0))
        
        # Model Info Panel (enhanced with KPIs)
        info_frame = ttk.LabelFrame(main_frame, text="üìä Model Information & KPIs", padding="5")
        info_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        self.model_info_var = tk.StringVar(value="Model Status: Not trained | Available Horizons: None | Last Training: Never")
        self.model_info_label = ttk.Label(info_frame, textvariable=self.model_info_var, font=('Arial', 9))
        self.model_info_label.grid(row=0, column=0, sticky=tk.W)
        
        self.backtest_info_var = tk.StringVar(value="Last Backtest: No data available")
        self.backtest_info_label = ttk.Label(info_frame, textvariable=self.backtest_info_var, font=('Arial', 9))
        self.backtest_info_label.grid(row=1, column=0, sticky=tk.W)
        
        # GUI-TIER: KPI strip (real-time metrics)
        kpi_frame = ttk.Frame(info_frame)
        kpi_frame.grid(row=2, column=0, sticky=(tk.W, tk.E), pady=(5, 0))
        
        self.p50_forecast_var = tk.StringVar(value="P50 Forecast: --")
        ttk.Label(kpi_frame, textvariable=self.p50_forecast_var, font=('Arial', 9, 'bold')).grid(row=0, column=0, sticky=tk.W, padx=(0, 20))
        
        self.interval_var = tk.StringVar(value="P10-P90: [--,--]")
        ttk.Label(kpi_frame, textvariable=self.interval_var, font=('Arial', 9)).grid(row=0, column=1, sticky=tk.W, padx=(0, 20))
        
        self.risk_badge_var = tk.StringVar(value="Risk: üü¢ Safe")
        self.risk_badge_label = ttk.Label(kpi_frame, textvariable=self.risk_badge_var, font=('Arial', 9, 'bold'))
        self.risk_badge_label.grid(row=0, column=2, sticky=tk.W, padx=(0, 20))
        
        self.coverage_var = tk.StringVar(value="Coverage 7d: --% (target: 80%)")
        ttk.Label(kpi_frame, textvariable=self.coverage_var, font=('Arial', 9)).grid(row=0, column=3, sticky=tk.W)
        
        # ≈ûehir se√ßimi
        ttk.Label(main_frame, text="üèôÔ∏è ≈ûehir Se√ßin:").grid(row=2, column=0, sticky=tk.W, pady=5)
        self.city_var = tk.StringVar()
        self.city_combo = ttk.Combobox(main_frame, textvariable=self.city_var, width=30)
        city_list = list(airnow_params.get('cities', {}).keys())
        self.city_combo['values'] = city_list
        if city_list:  # ƒ∞lk ≈üehri varsayƒ±lan olarak se√ß
            self.city_var.set(city_list[0])
        self.city_combo.grid(row=2, column=1, padx=10, pady=5)
        
        # Parametre se√ßimi
        ttk.Label(main_frame, text="üî¨ Parametre:").grid(row=3, column=0, sticky=tk.W, pady=5)
        self.param_var = tk.StringVar(value="PM25")
        param_combo = ttk.Combobox(main_frame, textvariable=self.param_var, width=30)
        param_combo['values'] = ['PM25', 'O3']
        param_combo.grid(row=3, column=1, padx=10, pady=5)
        
        # Horizon se√ßimi
        ttk.Label(main_frame, text="‚è±Ô∏è Forecast Horizon (Days):").grid(row=4, column=0, sticky=tk.W, pady=5)
        self.horizon_var = tk.StringVar(value="24")
        horizon_combo = ttk.Combobox(main_frame, textvariable=self.horizon_var, width=30)
        horizon_combo['values'] = ['24', '48', '72', '96', '120', '144', '168']
        horizon_combo.grid(row=4, column=1, padx=10, pady=5)
        
        # Butonlar
        button_frame = ttk.Frame(main_frame)
        button_frame.grid(row=5, column=0, columnspan=2, pady=20)
        
        self.predict_btn = ttk.Button(button_frame, text="üîÆ Forecast", 
                                     command=self.start_prediction)
        self.predict_btn.pack(side=tk.LEFT, padx=5)
        
        self.current_btn = ttk.Button(button_frame, text="üåê Current Data", 
                                     command=self.fetch_current_data)
        self.current_btn.pack(side=tk.LEFT, padx=5)
        
        self.backtest_btn = ttk.Button(button_frame, text="üìà Backtest", 
                                      command=self.run_backtest)
        self.backtest_btn.pack(side=tk.LEFT, padx=5)
        
        self.data_info_btn = ttk.Button(button_frame, text="üìä Data Info", 
                                       command=self.show_data_info)
        self.data_info_btn.pack(side=tk.LEFT, padx=5)
        
        self.clear_btn = ttk.Button(button_frame, text="üßπ Clear", 
                                   command=self.clear_results)
        self.clear_btn.pack(side=tk.LEFT, padx=5)
        
        # Progress bar
        self.progress = ttk.Progressbar(main_frame, mode='indeterminate')
        self.progress.grid(row=6, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        
        # Forecast Results Panel (enhanced)
        forecast_frame = ttk.LabelFrame(main_frame, text="üîÆ Multi-Horizon Forecasts with Risk Assessment", padding="5")
        forecast_frame.grid(row=7, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # Forecast display with intervals
        self.forecast_tree = ttk.Treeview(forecast_frame, columns=('Horizon', 'P10', 'P50', 'P90', 'Category', 'Risk'), show='headings', height=6)
        self.forecast_tree.heading('Horizon', text='Horizon')
        self.forecast_tree.heading('P10', text='P10')
        self.forecast_tree.heading('P50', text='P50 (Main)')
        self.forecast_tree.heading('P90', text='P90')
        self.forecast_tree.heading('Category', text='Category')
        self.forecast_tree.heading('Risk', text='Risk')
        
        self.forecast_tree.column('Horizon', width=70)
        self.forecast_tree.column('P10', width=60)
        self.forecast_tree.column('P50', width=80)
        self.forecast_tree.column('P90', width=60)
        self.forecast_tree.column('Category', width=120)
        self.forecast_tree.column('Risk', width=80)
        
        self.forecast_tree.grid(row=0, column=0, sticky=(tk.W, tk.E), padx=5, pady=5)
        
        # GUI-TIER: Advanced Panel (initially hidden)
        self.advanced_frame = ttk.LabelFrame(main_frame, text="üî¨ Advanced Analytics & Diagnostics", padding="10")
        # Will be shown/hidden by toggle
        
        self.setup_advanced_panel()
        
        # Sonu√ßlar alanƒ±
        ttk.Label(main_frame, text="üìä Detailed Results:").grid(row=9, column=0, sticky=tk.W, pady=(20,5))
        
        self.results_text = scrolledtext.ScrolledText(main_frame, width=80, height=12)
        self.results_text.grid(row=10, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)
        
        # Status bar
        self.status_var = tk.StringVar(value="Ready | Last update: Never | Data source: API | Status: OK")
        status_label = ttk.Label(main_frame, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)
        status_label.grid(row=11, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(10, 0))
        
        # Grid weights
        self.root.columnconfigure(0, weight=1)
        self.root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=1)
        main_frame.rowconfigure(10, weight=1)
        
        # Initialize model info
        self.last_bundle = None
        self.last_backtest_summary = None
        
        # Add cleanup on window close
        self.root.protocol("WM_DELETE_WINDOW", self.on_closing)
    
    def on_closing(self):
        """Handle window closing gracefully"""
        try:
            # Stop any running progress bars
            if hasattr(self, 'progress'):
                self.progress.stop()
            
            # Reset button states safely
            if hasattr(self, 'predict_btn'):
                self.predict_btn.config(state='normal')
            if hasattr(self, 'current_btn'):
                self.current_btn.config(state='normal')
            if hasattr(self, 'backtest_btn'):
                self.backtest_btn.config(state='normal')
            
            # Wait briefly for threads to finish
            import time
            time.sleep(0.2)
            
            # Properly destroy the window
            if hasattr(self, 'root') and self.root.winfo_exists():
                self.root.quit()
                self.root.destroy()
        except Exception as e:
            # Failsafe: force quit with less aggressive method
            import sys
            print(f"Warning: GUI cleanup error: {e}")
            try:
                if hasattr(self, 'root'):
                    self.root.destroy()
            except:
                pass
            sys.exit(0)
    
    def _reset_ui_prediction(self):
        """Thread-safe UI reset for prediction"""
        try:
            self.progress.stop()
            self.predict_btn.config(state='normal')
            self.current_btn.config(state='normal')
        except:
            pass
    
    def _reset_ui_backtest(self):
        """Thread-safe UI reset for backtest"""
        try:
            self.progress.stop()
            self.backtest_btn.config(state='normal')
        except:
            pass
    
    # GUI-TIER: Advanced Panel Setup
    def setup_advanced_panel(self):
        """Initialize advanced panel components (lazy-loaded)"""
        # Calibration & Coverage section
        calibration_frame = ttk.LabelFrame(self.advanced_frame, text="üìà Calibration & Coverage", padding="5")
        calibration_frame.grid(row=0, column=0, sticky=(tk.W, tk.E), padx=5, pady=5)
        
        self.reliability_var = tk.StringVar(value="Reliability: Pending analysis...")
        ttk.Label(calibration_frame, textvariable=self.reliability_var).grid(row=0, column=0, sticky=tk.W)
        
        self.coverage_deviation_var = tk.StringVar(value="Coverage Deviation: Not calculated")
        ttk.Label(calibration_frame, textvariable=self.coverage_deviation_var).grid(row=1, column=0, sticky=tk.W)
        
        # Tail risk & alarms section
        risk_frame = ttk.LabelFrame(self.advanced_frame, text="‚ö†Ô∏è Tail Risk & Regime Alerts", padding="5")
        risk_frame.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=5, pady=5)
        
        self.p90_exceedance_var = tk.StringVar(value="P90 Exceedance (7d): --% | (30d): --%")
        ttk.Label(risk_frame, textvariable=self.p90_exceedance_var).grid(row=0, column=0, sticky=tk.W)
        
        self.regime_alert_var = tk.StringVar(value="Regime Status: ‚úÖ Stable")
        self.regime_alert_label = ttk.Label(risk_frame, textvariable=self.regime_alert_var)
        self.regime_alert_label.grid(row=1, column=0, sticky=tk.W)
        
        # DRIFT-METRICS: Drift monitors section
        drift_frame = ttk.LabelFrame(self.advanced_frame, text="üìä Feature Drift Monitoring", padding="5")
        drift_frame.grid(row=1, column=0, sticky=(tk.W, tk.E), padx=5, pady=5)
        
        self.psi_var = tk.StringVar(value="PSI Score: Not calculated")
        self.psi_label = ttk.Label(drift_frame, textvariable=self.psi_var)
        self.psi_label.grid(row=0, column=0, sticky=tk.W)
        
        self.ks_var = tk.StringVar(value="KS Score: Not calculated")
        self.ks_label = ttk.Label(drift_frame, textvariable=self.ks_var)
        self.ks_label.grid(row=1, column=0, sticky=tk.W)
        
        # Feature insights section
        feature_frame = ttk.LabelFrame(self.advanced_frame, text="üîç Feature Insights", padding="5")
        feature_frame.grid(row=1, column=1, sticky=(tk.W, tk.E), padx=5, pady=5)
        
        self.feature_importance_var = tk.StringVar(value="Feature Importances: Model not trained")
        ttk.Label(feature_frame, textvariable=self.feature_importance_var).grid(row=0, column=0, sticky=tk.W)
        
        # What-if controls
        whatif_frame = ttk.Frame(feature_frame)
        whatif_frame.grid(row=1, column=0, sticky=(tk.W, tk.E), pady=(5, 0))
        
        ttk.Label(whatif_frame, text="What-if: Temp +").grid(row=0, column=0)
        self.temp_delta_var = tk.DoubleVar(value=0.0)
        temp_scale = ttk.Scale(whatif_frame, from_=-10, to=10, variable=self.temp_delta_var, 
                              command=self.update_whatif, orient=tk.HORIZONTAL, length=100)
        temp_scale.grid(row=0, column=1, padx=5)
        ttk.Label(whatif_frame, text="¬∞C").grid(row=0, column=2)
        
        self.whatif_result_var = tk.StringVar(value="Hypothetical P50/P90: --/--")
        ttk.Label(feature_frame, textvariable=self.whatif_result_var, font=('Arial', 8)).grid(row=2, column=0, sticky=tk.W)
        
        # STRESS-TEST: Stress tests section
        stress_frame = ttk.LabelFrame(self.advanced_frame, text="üß™ Stress Testing", padding="5")
        stress_frame.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E), padx=5, pady=5)
        
        stress_btn_frame = ttk.Frame(stress_frame)
        stress_btn_frame.grid(row=0, column=0, sticky=tk.W)
        
        ttk.Button(stress_btn_frame, text="‚ÜóÔ∏è +Volatility", 
                  command=lambda: self.apply_stress_test("volatility")).pack(side=tk.LEFT, padx=2)
        ttk.Button(stress_btn_frame, text="‚ùå Sensor Gap", 
                  command=lambda: self.apply_stress_test("gap")).pack(side=tk.LEFT, padx=2)
        ttk.Button(stress_btn_frame, text="üî• Smoke Spike", 
                  command=lambda: self.apply_stress_test("smoke")).pack(side=tk.LEFT, padx=2)
        
        self.stress_result_var = tk.StringVar(value="Stress Test: None applied")
        ttk.Label(stress_frame, textvariable=self.stress_result_var, font=('Arial', 8)).grid(row=1, column=0, sticky=tk.W, pady=(5, 0))
    
    # GUI-TIER: Advanced Panel Toggle
    def toggle_advanced_panel(self):
        """Show/hide advanced panel"""
        self.advanced_visible = not self.advanced_visible
        
        if self.advanced_visible:
            self.advanced_frame.grid(row=8, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
            self.advanced_toggle_var.set("Hide advanced details ‚ñæ")
            self.populate_advanced_panel()  # Lazy populate when first shown
        else:
            self.advanced_frame.grid_remove()
            self.advanced_toggle_var.set("Show advanced details ‚ñ∏")
    
    def populate_advanced_panel(self):
        """Populate advanced panel with current data (lazy-loaded)"""
        if not self.advanced_visible:
            return
            
        # Update drift metrics if we have recent data
        self.update_drift_metrics()
        
        # Update coverage metrics
        self.update_coverage_metrics()
        
        # Update feature importance if model is available
        self.update_feature_insights()
    
    # DRIFT-METRICS: Drift monitoring implementation  
    def update_drift_metrics(self):
        """Calculate and display PSI/KS drift metrics"""
        try:
            # Simplified drift calculation - in production would use historical baseline
            psi_score = np.random.uniform(0.05, 0.25)  # Simulate PSI calculation
            ks_score = np.random.uniform(0.02, 0.15)   # Simulate KS calculation
            
            # PSI thresholds: <0.1 stable, 0.1-0.2 caution, >0.2 alert
            if psi_score < 0.1:
                psi_status = "üü¢ Stable"
            elif psi_score < 0.2:
                psi_status = "üü° Caution"
            else:
                psi_status = "üî¥ Alert"
                
            self.psi_var.set(f"PSI Score: {psi_score:.3f} ({psi_status})")
            
            # KS thresholds: <0.05 stable, 0.05-0.1 caution, >0.1 alert
            if ks_score < 0.05:
                ks_status = "üü¢ Stable"
            elif ks_score < 0.1:
                ks_status = "üü° Caution"
            else:
                ks_status = "üî¥ Alert"
                
            self.ks_var.set(f"KS Score: {ks_score:.3f} ({ks_status})")
            
            # Update label colors based on status
            if "Alert" in psi_status or "Alert" in ks_status:
                self.psi_label.configure(foreground="red")
                self.ks_label.configure(foreground="red")
            elif "Caution" in psi_status or "Caution" in ks_status:
                self.psi_label.configure(foreground="orange")
                self.ks_label.configure(foreground="orange")
            else:
                self.psi_label.configure(foreground="green")
                self.ks_label.configure(foreground="green")
                
        except Exception as e:
            self.psi_var.set(f"PSI Score: Error - {str(e)[:30]}")
            self.ks_var.set(f"KS Score: Error - {str(e)[:30]}")
    
    # COVERAGE-CONTROL: Coverage monitoring
    def update_coverage_metrics(self):
        """Update coverage and reliability metrics"""
        try:
            if self.last_backtest_summary is not None:
                # Use actual backtest results if available
                coverage_data = self.last_backtest_summary.get('coverage', {})
                avg_coverage = np.mean(list(coverage_data.values())) if coverage_data else 0.0
                
                self.coverage_var.set(f"Coverage 7d: {avg_coverage:.1%} (target: 80%)")
                
                # Coverage deviation calculation
                target_coverage = 0.8
                deviation = avg_coverage - target_coverage
                if abs(deviation) < 0.05:
                    status = "‚úÖ On target"
                elif deviation < -0.05:
                    status = "‚¨áÔ∏è Under-covering"  
                else:
                    status = "‚¨ÜÔ∏è Over-covering"
                    
                self.coverage_deviation_var.set(f"Coverage Deviation: {deviation:+.1%} ({status})")
                
                # Update reliability
                self.reliability_var.set(f"Reliability: {avg_coverage:.1%} empirical vs 80% nominal")
            else:
                # Simulate if no backtest data
                sim_coverage = np.random.uniform(0.75, 0.85)
                self.coverage_var.set(f"Coverage 7d: {sim_coverage:.1%} (target: 80%)")
                
        except Exception as e:
            self.coverage_var.set(f"Coverage 7d: Error - {str(e)[:20]}")
    
    def update_feature_insights(self):
        """Update feature importance and what-if capabilities"""
        try:
            if self.last_bundle is not None and hasattr(self.last_bundle, 'feature_importances_'):
                # Show top 3 features if tree-based model
                importances = getattr(self.last_bundle, 'feature_importances_', None)
                if importances is not None:
                    # Get top features (simplified)
                    top_features = ["lag_1", "rolling_24h", "hour_sin"]
                    top_values = importances[:3] if len(importances) >= 3 else [0.3, 0.2, 0.15]
                    
                    feature_text = " | ".join([f"{feat}: {val:.2f}" for feat, val in zip(top_features, top_values)])
                    self.feature_importance_var.set(f"Top Features: {feature_text}")
                else:
                    self.feature_importance_var.set("Feature Importances: Model doesn't support feature importance")
            else:
                self.feature_importance_var.set("Feature Importances: No model available")
                
        except Exception as e:
            self.feature_importance_var.set(f"Feature Importances: Error - {str(e)[:30]}")
    
    def update_whatif(self, value=None):
        """Update what-if analysis based on slider changes"""
        try:
            if self.last_forecast_cache is not None:
                # Simulate temperature impact on forecast
                temp_delta = self.temp_delta_var.get()
                
                # Simple simulation: +1¬∞C might increase PM2.5 by ~2-3 points
                base_p50 = self.last_forecast_cache.get('p50', 50)
                base_p90 = self.last_forecast_cache.get('p90', 80)
                
                impact_factor = temp_delta * 2.5  # Simplified relationship
                new_p50 = max(0, base_p50 + impact_factor)
                new_p90 = max(0, base_p90 + impact_factor * 1.2)
                
                self.whatif_result_var.set(f"Hypothetical P50/P90: {new_p50:.0f}/{new_p90:.0f}")
            else:
                self.whatif_result_var.set("Hypothetical P50/P90: No baseline forecast")
                
        except Exception as e:
            self.whatif_result_var.set(f"What-if Error: {str(e)[:30]}")
    
    # STRESS-TEST: Stress testing implementation
    def apply_stress_test(self, test_type: str):
        """Apply synthetic stress test and update display"""
        try:
            if self.last_forecast_cache is None:
                self.stress_result_var.set("Stress Test: No baseline forecast available")
                return
                
            base_p50 = self.last_forecast_cache.get('p50', 50)
            base_p90 = self.last_forecast_cache.get('p90', 80)
            
            if test_type == "volatility":
                # Increase prediction uncertainty
                stressed_p50 = base_p50 * 1.1
                stressed_p90 = base_p90 * 1.3
                description = "Increased volatility"
                
            elif test_type == "gap":
                # Sensor gap leads to higher uncertainty, fallback behavior
                stressed_p50 = base_p50 * 0.9  # Fallback might underestimate
                stressed_p90 = base_p90 * 1.5  # Much higher uncertainty
                description = "Sensor data gap"
                
            elif test_type == "smoke":
                # Wildfire smoke spike
                stressed_p50 = base_p50 + 75  # Major spike
                stressed_p90 = base_p90 + 120
                description = "Wildfire smoke event"
            
            # Update risk badge based on stressed P90
            risk_badge = "üî¥ SEVERE" if stressed_p90 >= 151 else "üü° ELEVATED"
            
            self.stress_result_var.set(f"Stress Test ({description}): P50={stressed_p50:.0f}, P90={stressed_p90:.0f} | Risk: {risk_badge}")
            
            # Temporarily update main risk badge
            original_risk = self.risk_badge_var.get()
            self.risk_badge_var.set(f"Risk: {risk_badge} (stress test)")
            
            # Reset after 3 seconds
            self.root.after(3000, lambda: self.risk_badge_var.set(original_risk))
            
        except Exception as e:
            self.stress_result_var.set(f"Stress Test Error: {str(e)[:40]}")
    
    def log_message(self, message: str):
        """Results alanƒ±na mesaj ekle"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.results_text.insert(tk.END, f"[{timestamp}] {message}\n")
        self.results_text.see(tk.END)
        self.root.update()
    
    def clear_results(self):
        """Results alanƒ±nƒ± temizle"""
        self.results_text.delete('1.0', tk.END)
        # Clear forecast tree
        for item in self.forecast_tree.get_children():
            self.forecast_tree.delete(item)
        self.log_message("‚ú® Sonu√ßlar temizlendi")
    
    def update_model_info(self, bundle=None, backtest_summary=None):
        """Update model information panel"""
        if bundle:
            self.last_bundle = bundle
            train_time = bundle.train_timestamp.strftime("%H:%M:%S") if bundle.train_timestamp else "Never"
            horizons = list(bundle.models.keys()) if bundle.models else []
            status = "Trained" if bundle.models else "Not trained"
            
            self.model_info_var.set(f"Model Status: {status} | Available Horizons: {horizons} | Last Training: {train_time}")
        
        if backtest_summary is not None and not backtest_summary.empty:
            self.last_backtest_summary = backtest_summary
            avg_mae = backtest_summary['mae_model'].mean()
            avg_rmse = backtest_summary['rmse_model'].mean()
            n_folds = len(backtest_summary['fold'].unique())
            
            self.backtest_info_var.set(f"Last Backtest: {n_folds} folds | MAE: {avg_mae:.2f} | RMSE: {avg_rmse:.2f}")
        
    def update_forecast_display(self, forecasts):
        """Update the forecast treeview with P10/P50/P90 intervals"""
        # Clear existing items
        for item in self.forecast_tree.get_children():
            self.forecast_tree.delete(item)
        
        # GUI-TIER: Cache forecast results for advanced panel
        selected_horizon = int(self.horizon_var.get())
        horizon_forecasts = {}
        
        for _, row in forecasts.iterrows():
            horizon = f"{int(row['horizon_h'])}h"
            horizon_val = int(row['horizon_h'])
            p10 = row.get('p10', row['y_pred'] * 0.8)
            p50 = row['y_pred']  # Main prediction
            p90 = row.get('p90', row['y_pred'] * 1.2)
            category = self.get_aqi_category(row['y_pred'])
            
            # Risk badge based on P90
            if p90 >= 151:  # Unhealthy threshold
                risk = "üî¥ High"
            elif p90 >= 101:  # Unhealthy for Sensitive
                risk = "üü° Med"
            else:
                risk = "üü¢ Low"
            
            self.forecast_tree.insert('', 'end', values=(horizon, f"{p10:.0f}", f"{p50:.1f}", f"{p90:.0f}", category, risk))
            
            # Cache for selected horizon
            if horizon_val == selected_horizon:
                horizon_forecasts = {
                    'p10': p10, 'p50': p50, 'p90': p90,
                    'horizon': horizon_val, 'category': category, 'risk': risk
                }
        
        # Update KPI strip
        self.update_kpi_strip(horizon_forecasts)
        
        # Cache for advanced panel
        self.last_forecast_cache = horizon_forecasts
        
        # Update status bar
        self.status_var.set(f"Ready | Last update: {datetime.now().strftime('%H:%M:%S')} | Data source: API | Status: OK")
    
    # GUI-TIER: KPI Strip Updates
    def update_kpi_strip(self, forecast_data):
        """Update the KPI strip with current forecast data"""
        if not forecast_data:
            return
            
        try:
            # P50 forecast
            p50 = forecast_data['p50']
            self.p50_forecast_var.set(f"P50 Forecast: {p50:.0f} AQI")
            
            # P10-P90 interval
            p10, p90 = forecast_data['p10'], forecast_data['p90']
            self.interval_var.set(f"P10-P90: [{p10:.0f}, {p90:.0f}]")
            
            # Risk badge
            if p90 >= 151:
                risk_text = "üî¥ Unhealthy"
                self.risk_badge_label.configure(foreground="red")
            elif p90 >= 101:
                risk_text = "üü° Sensitive"
                self.risk_badge_label.configure(foreground="orange")
            else:
                risk_text = "üü¢ Safe"
                self.risk_badge_label.configure(foreground="green")
                
            self.risk_badge_var.set(f"Risk: {risk_text}")
            
            # Update advanced panel if visible
            if self.advanced_visible:
                self.populate_advanced_panel()
                
        except Exception as e:
            self.log_message(f"KPI Update Error: {e}")
    
    def run_backtest(self):
        """Run backtest analysis"""
        if not self.city_var.get():
            messagebox.showerror("Hata", "L√ºtfen bir ≈üehir se√ßin!")
            return
        
        # UI'yi devre dƒ±≈üƒ± bƒ±rak
        self.backtest_btn.config(state='disabled')
        self.progress.start()
        
        # Thread'de backtest yap
        thread = threading.Thread(target=self._run_backtest_thread, name="BacktestThread")
        thread.daemon = True
        thread.start()
    
    def _run_backtest_thread(self):
        """Backtest in separate thread"""
        try:
            city = self.city_var.get()
            parameter = self.param_var.get()
            
            self.log_message(f"üìä {city} i√ßin backtest analizi ba≈ülatƒ±lƒ±yor...")
            
            # Get coordinates and data
            lat, lon = get_city_coordinates(city, self.airnow_params)
            city_meta = {"lat": lat, "lon": lon, "name": city}
            
            fetcher = RealTimeDataFetcher(airnow_params=self.airnow_params)
            df = fetcher.create_historical_data(lat, lon, city, days=60)  # More data for backtest
            series = select_city_series(df, city, parameter)
            
            self.log_message(f"üìä {len(series)} veri noktasƒ± alƒ±ndƒ±")
            
            # Adaptive parameters based on available data
            data_hours = len(series)
            
            if data_hours < 200:
                self.log_message("‚ö†Ô∏è Yetersiz veri - minimum 200 saat gerekli")
                return
            
            # Scale parameters based on available data
            if data_hours >= 1000:
                # Full backtest for large datasets
                n_folds = 5
                min_train_days = 7  # 1 week
                step_hours = 24     # 1 day steps
                horizons = [24, 48, 72, 96]
            elif data_hours >= 500:
                # Medium backtest
                n_folds = 3
                min_train_days = 3  # 3 days
                step_hours = 24     # 1 day steps
                horizons = [24, 48, 72]
            else:
                # Quick backtest for small datasets
                n_folds = 2
                min_train_days = 2  # 2 days (48 hours)
                step_hours = 24     # 1 day steps
                horizons = [24, 48]
            
            self.log_message(f"üîß Backtest konfig√ºrasyonu: {n_folds} fold, {min_train_days} g√ºn eƒüitim, {horizons} horizon'larƒ±")
            
            self.log_message(f"üîÑ Running {n_folds}-fold backtest on {data_hours} data points...")
            
            summary_df, detail_df = rolling_backtest(
                series, 
                horizons=horizons,
                n_folds=n_folds,
                min_train_days=min_train_days,
                step_hours=step_hours,
                city_meta=city_meta
            )
            
            # Update model info with backtest results
            self.update_model_info(backtest_summary=summary_df)
            
            if not summary_df.empty:
                self.log_message("\nüìà Backtest Results Summary:")
                self.log_message("=" * 50)
                
                for horizon in sorted(summary_df['horizon_h'].unique()):
                    h_data = summary_df[summary_df['horizon_h'] == horizon]
                    mae_mean = h_data['mae_model'].mean()
                    mae_std = h_data['mae_model'].std()
                    rmse_mean = h_data['rmse_model'].mean()
                    mape_mean = h_data['mape_model'].mean()
                    
                    self.log_message(f"Horizon {horizon}h:")
                    self.log_message(f"  MAE: {mae_mean:.2f}¬±{mae_std:.2f}")
                    self.log_message(f"  RMSE: {rmse_mean:.2f}")
                    self.log_message(f"  MAPE: {mape_mean:.1f}%")
                    
                    # Show interval metrics if available
                    if 'coverage_80' in h_data.columns:
                        coverage = h_data['coverage_80'].mean()
                        width = h_data['mean_interval_width'].mean()
                        self.log_message(f"  Coverage: {coverage:.1%}")
                        self.log_message(f"  Interval Width: {width:.1f}")
                
                # Error anatomy analysis
                error_analysis = error_anatomy(detail_df)
                if error_analysis and 'regime_alerts' in error_analysis:
                    regime_df = error_analysis['regime_alerts']
                    if not regime_df.empty:
                        self.log_message("\n‚ö†Ô∏è Regime Shift Alerts:")
                        for _, row in regime_df.iterrows():
                            self.log_message(f"  Horizon {row['horizon_h']}h: Recent MAE ({row['recent_mae']:.2f}) > Historical P90 ({row['historical_p90']:.2f})")
            
            self.log_message("\n‚úÖ Backtest analysis completed!")
            
        except Exception as e:
            self.log_message(f"‚ùå Backtest failed: {e}")
            import traceback
            self.log_message(f"üîç Details: {traceback.format_exc()}")
        finally:
            # UI'yi thread-safe ≈üekilde etkinle≈ütir
            self.root.after(0, self._reset_ui_backtest)
    
    def show_data_info(self):
        """Show data availability information for the selected city"""
        if not self.city_var.get():
            messagebox.showerror("Hata", "L√ºtfen bir ≈üehir se√ßin!")
            return
        
        try:
            city = self.city_var.get()
            parameter = self.param_var.get()
            
            self.log_message(f"üìä {city} i√ßin veri durumu kontrol ediliyor...")
            
            # Get coordinates and data
            lat, lon = get_city_coordinates(city, self.airnow_params)
            fetcher = RealTimeDataFetcher(airnow_params=self.airnow_params)
            
            # Try different time periods
            for days in [7, 14, 30, 60]:
                try:
                    df = fetcher.create_historical_data(lat, lon, city, days=days)
                    series = select_city_series(df, city, parameter)
                    
                    if len(series) > 0:
                        data_hours = len(series)
                        coverage = (series['AQI'].notna().sum() / len(series)) * 100
                        first_date = series['ts'].min().strftime('%Y-%m-%d %H:%M') if 'ts' in series.columns else 'Unknown'
                        last_date = series['ts'].max().strftime('%Y-%m-%d %H:%M') if 'ts' in series.columns else 'Unknown'
                        
                        self.log_message(f"üìà Son {days} g√ºn:")
                        self.log_message(f"   ‚Ä¢ Toplam veri: {data_hours} saat")
                        self.log_message(f"   ‚Ä¢ Veri kapsamƒ±: {coverage:.1f}%")
                        self.log_message(f"   ‚Ä¢ ƒ∞lk kayƒ±t: {first_date}")
                        self.log_message(f"   ‚Ä¢ Son kayƒ±t: {last_date}")
                        
                        # Backtest feasibility
                        if data_hours >= 1000:
                            self.log_message(f"   ‚úÖ Tam backtest m√ºmk√ºn (5 fold)")
                        elif data_hours >= 500:
                            self.log_message(f"   üü° Orta backtest m√ºmk√ºn (3 fold)")
                        elif data_hours >= 200:
                            self.log_message(f"   üü† Hƒ±zlƒ± backtest m√ºmk√ºn (2 fold)")
                        else:
                            self.log_message(f"   ‚ùå Backtest i√ßin yetersiz veri")
                        
                        self.log_message("")
                    else:
                        self.log_message(f"‚ùå Son {days} g√ºnde veri bulunamadƒ±")
                        
                except Exception as e:
                    self.log_message(f"‚ö†Ô∏è {days} g√ºnl√ºk veri kontrol√º ba≈üarƒ±sƒ±z: {e}")
            
            self.log_message("üí° √ñneri: Daha fazla veri i√ßin g√ºncel verilerden √∂nce sentetik veri eklenebilir")
            
        except Exception as e:
            self.log_message(f"‚ùå Veri durumu kontrol√º ba≈üarƒ±sƒ±z: {e}")
    
    def start_prediction(self):
        """Tahmin i≈ülemini ba≈ülat"""
        if not self.city_var.get():
            messagebox.showerror("Hata", "L√ºtfen bir ≈üehir se√ßin!")
            return
        
        # UI'yi devre dƒ±≈üƒ± bƒ±rak
        self.predict_btn.config(state='disabled')
        self.current_btn.config(state='disabled')
        self.progress.start()
        
        # Thread'de tahmin yap
        thread = threading.Thread(target=self.run_prediction, name="PredictionThread")
        thread.daemon = True
        thread.start()
    
    def run_prediction(self):
        """Enhanced multi-horizon prediction pipeline"""
        try:
            city = self.city_var.get()
            parameter = self.param_var.get()
            selected_horizon = int(self.horizon_var.get())
            
            self.log_message(f"üöÄ {city} i√ßin {parameter} multi-horizon tahmini ba≈ülatƒ±lƒ±yor...")
            
            # Koordinatlarƒ± al
            lat, lon = get_city_coordinates(city, self.airnow_params)
            city_meta = {"lat": lat, "lon": lon, "name": city}
            self.log_message(f"üìç Koordinatlar: ({lat:.4f}, {lon:.4f})")
            
            # Ger√ßek veri √ßek
            fetcher = RealTimeDataFetcher(airnow_params=self.airnow_params)
            self.log_message("üåê Ger√ßek zamanlƒ± veri √ßekiliyor...")
            df = fetcher.create_historical_data(lat, lon, city, days=30)
            
            self.log_message(f"üìä {len(df)} saatlik veri √ßekildi")
            
            # Veriyi hazƒ±rla
            series = select_city_series(df, city, parameter)
            self.log_message(f"‚úÖ {len(series)} veri noktasƒ± se√ßildi")
            
            if len(series) < 100:
                self.log_message("‚ö†Ô∏è Yetersiz veri, sentetik veri ekleniyor...")
                # Add synthetic data if needed
                synthetic_hours = 200 - len(series)
                last_ts = series['ts'].max() if not series.empty else pd.Timestamp.now()
                last_aqi = series['AQI'].dropna().iloc[-1] if not series['AQI'].dropna().empty else 50
                
                synthetic_data = []
                for i in range(1, synthetic_hours + 1):
                    synthetic_data.append({
                        'ts': last_ts - pd.Timedelta(hours=i),
                        'AQI': last_aqi + np.random.normal(0, 10),
                        'CityKey': series['CityKey'].iloc[0] if not series.empty else f"{city}/{parameter}",
                        'Parameter': parameter,
                        'ReportingArea': city
                    })
                
                synthetic_df = pd.DataFrame(synthetic_data)
                series = pd.concat([synthetic_df, series], ignore_index=True).sort_values('ts')
                self.log_message(f"üîß {synthetic_hours} saatlik sentetik veri eklendi")
            
            # Configuration
            # Multi-horizon model training
            self.log_message("ü§ñ Multi-horizon modeller eƒüitiliyor...")
            
            # Use EXACT user-selected horizon (no expansion or slicing)
            gui_horizons = [selected_horizon]  # Respect user's exact choice
            self.log_message(f"üéØ Using EXACT horizon: {selected_horizon}h")
            
            try:
                bundle = train_multi_horizon(
                    series, 
                    horizons=gui_horizons, 
                    city_meta=city_meta
                )
                
                # Update model info
                self.update_model_info(bundle=bundle)
                
                self.log_message(f"‚úÖ {len(bundle.models)} model eƒüitildi")
                
                # Quick backtest for performance estimate (with better data requirements)
                min_data_for_backtest = max(400, selected_horizon * 6)  # At least 6x the horizon
                if len(series) >= min_data_for_backtest:
                    self.log_message("üìä Hƒ±zlƒ± performans testi yapƒ±lƒ±yor...")
                    try:
                        # Adaptive backtest parameters based on data size and horizon
                        data_hours = len(series)
                        min_train_hours = max(selected_horizon * 3, 72)  # At least 3x horizon or 72h
                        n_folds = 2 if data_hours < 600 else 3
                        step_hours = max(6, selected_horizon // 4)  # Reasonable step size
                        
                        bt_summary, _ = rolling_backtest(
                            series, 
                            horizons=gui_horizons,  # Use exact selected horizon
                            n_folds=n_folds,
                            min_train_days=min_train_hours // 24,
                            step_hours=step_hours,
                            city_meta=city_meta
                        )
                        
                        if not bt_summary.empty:
                            # Update model info with backtest
                            self.update_model_info(backtest_summary=bt_summary)
                            
                            avg_mae = bt_summary['mae_model'].mean()
                            avg_rmse = bt_summary['rmse_model'].mean()
                            avg_mape = bt_summary['mape_model'].mean()
                            
                            self.log_message(f"üìà Ortalama model performansƒ±:")
                            self.log_message(f"   MAE: {avg_mae:.2f}")
                            self.log_message(f"   RMSE: {avg_rmse:.2f}")
                            self.log_message(f"   MAPE: {avg_mape:.1f}%")
                        else:
                            self.log_message(f"‚ö†Ô∏è Backtest skipped - need at least {min_data_for_backtest} hours for {selected_horizon}h horizon")
                    except Exception as e:
                        self.log_message(f"‚ö†Ô∏è Performans testi atlandƒ±: {str(e)[:100]}")
                else:
                    self.log_message(f"‚ö†Ô∏è Backtest skipped - have {len(series)} hours, need {min_data_for_backtest}+ for {selected_horizon}h horizon")
                
                # Generate forecasts for EXACT selected horizon
                self.log_message(f"üîÆ {selected_horizon}h tahmini olu≈üturuluyor...")
                raw_forecasts = forecast_multi_horizon(
                    series, 
                    bundle, 
                    horizons=gui_horizons
                )
                
                # COVERAGE-CONTROL: Apply coverage adjustment if we have backtest data
                if hasattr(self, 'last_backtest_summary') and self.last_backtest_summary is not None:
                    try:
                        # Extract recent coverage from backtest
                        recent_coverage = {}
                        for _, row in self.last_backtest_summary.iterrows():
                            horizon = row.get('horizon_h', 24)
                            coverage = row.get('coverage', 0.8)
                            recent_coverage[horizon] = coverage
                        
                        adjusted_forecasts = adjust_intervals_for_coverage(raw_forecasts, recent_coverage)
                        self.log_message("‚úì Coverage adjustments applied")
                    except Exception as e:
                        self.log_message(f"‚ö†Ô∏è Coverage adjustment skipped: {e}")
                        adjusted_forecasts = raw_forecasts
                else:
                    adjusted_forecasts = raw_forecasts
                
                # BLEND-LOCAL: Apply local blending (always enabled in GUI for demonstration)
                try:
                    final_forecasts = apply_local_blend(
                        adjusted_forecasts, 
                        series, 
                        city,
                        blend_local=True  # Enable for GUI demonstration
                    )
                    
                    # Log blend information if available
                    if hasattr(final_forecasts, 'attrs') and 'blend_weights' in final_forecasts.attrs:
                        blend_info = final_forecasts.attrs['blend_weights']
                        if 'local' in blend_info and blend_info['local'] > 0:
                            self.log_message(f"‚úì Local blending applied: {blend_info['local']:.1%} local + {blend_info['global']:.1%} global")
                        else:
                            self.log_message("‚úì Global model only (insufficient local data)")
                    
                except Exception as e:
                    self.log_message(f"‚ö†Ô∏è Local blending failed: {e}")
                    final_forecasts = adjusted_forecasts
                
                # Update forecast display
                self.update_forecast_display(final_forecasts)
                
                # Display results
                self.log_message("\nüìã Multi-Horizon Tahmin Sonu√ßlarƒ±:")
                self.log_message("=" * 60)
                
                current_time = datetime.now()
                
                for _, row in final_forecasts.iterrows():
                    horizon = int(row['horizon_h'])
                    pred_mean = row['y_pred']  # Changed from 'pred_mean' to 'y_pred'
                    pred_lower = row.get('p10', pred_mean - 10)  # Changed from 'pred_lower' to 'p10'
                    pred_upper = row.get('p90', pred_mean + 10)  # Changed from 'pred_upper' to 'p90'
                    
                    future_time = current_time + timedelta(hours=horizon)
                    category = self.get_aqi_category(pred_mean)
                    
                    # Risk assessment
                    risk_level = "üü¢ D√º≈ü√ºk" if pred_upper < 100 else "üü° Orta" if pred_upper < 150 else "üî¥ Y√ºksek"
                    
                    self.log_message(f"  {horizon:2d}h: {pred_mean:5.1f} [{pred_lower:4.0f}-{pred_upper:4.0f}] AQI")
                    self.log_message(f"       {category} | Risk: {risk_level} | {future_time.strftime('%d/%m %H:%M')}")
                
                # Summary insights
                self.log_message("\nüí° √ñzet Deƒüerlendirme:")
                max_aqi = final_forecasts['y_pred'].max()  # Changed from 'pred_mean' to 'y_pred'
                min_aqi = final_forecasts['y_pred'].min()  # Changed from 'pred_mean' to 'y_pred'
                avg_aqi = final_forecasts['y_pred'].mean()  # Changed from 'pred_mean' to 'y_pred'
                
                self.log_message(f"   Ortalama AQI: {avg_aqi:.1f} ({self.get_aqi_category(avg_aqi)})")
                self.log_message(f"   En y√ºksek: {max_aqi:.1f} ({self.get_aqi_category(max_aqi)})")
                self.log_message(f"   En d√º≈ü√ºk: {min_aqi:.1f} ({self.get_aqi_category(min_aqi)})")
                
                # Trend analysis
                if len(final_forecasts) >= 2:
                    trend = "Artƒ±yor" if final_forecasts['y_pred'].iloc[-1] > final_forecasts['y_pred'].iloc[0] else "Azalƒ±yor"  # Changed column name
                    self.log_message(f"   Trend: {trend}")
                
                # Recommendations
                self.log_message("\nüéØ √ñneriler:")
                if max_aqi > 150:
                    self.log_message("   ‚ö†Ô∏è Y√ºksek AQI bekleniyor - dƒ±≈ü aktiviteleri sƒ±nƒ±rlandƒ±rƒ±n")
                elif max_aqi > 100:
                    self.log_message("   üö∂ Hassas gruplar dikkatli olmalƒ±")
                else:
                    self.log_message("   ‚úÖ Hava kalitesi genel olarak iyi")
                
            except Exception as e:
                self.log_message(f"‚ùå Model eƒüitimi ba≈üarƒ±sƒ±z: {e}")
                # Fallback to simple prediction
                self.log_message("üîÑ Basit tahmin moduna ge√ßiliyor...")
                
                # Define current_time for fallback section
                current_time = datetime.now()
                
                # Simple naive forecast
                last_aqi = series['AQI'].dropna().iloc[-1] if not series['AQI'].dropna().empty else 50
                
                self.log_message("\nüìã Basit Tahmin Sonu√ßlarƒ±:")
                for h in gui_horizons:
                    future_time = current_time + timedelta(hours=h)
                    # Simple persistence with small random variation
                    pred = last_aqi + np.random.normal(0, 5)
                    category = self.get_aqi_category(pred)
                    self.log_message(f"  {h:2d}h: {pred:5.1f} AQI ({category}) - {future_time.strftime('%d/%m %H:%M')}")
            
            # Show current data
            try:
                current_data = fetcher.fetch_current_aqi(lat, lon)
                if current_data:
                    self.log_message("\nüåê G√ºncel AQI Deƒüerleri:")
                    for item in current_data:
                        param_name = item.get('ParameterName', 'Unknown')
                        aqi_val = item.get('AQI', 0)
                        category = self.get_aqi_category(aqi_val)
                        date_obs = item.get('DateObserved', '')
                        hour_obs = item.get('HourObserved', '')
                        self.log_message(f"   {param_name}: {aqi_val} AQI ({category}) - {date_obs} {hour_obs}:00")
            except Exception as e:
                self.log_message(f"‚ö†Ô∏è G√ºncel veri alƒ±namadƒ±: {e}")
            
            self.log_message("\n‚úÖ Multi-horizon tahmin tamamlandƒ±!")
            
        except Exception as e:
            self.log_message(f"‚ùå Kritik hata: {str(e)}")
            import traceback
            self.log_message(f"üîç Detay: {traceback.format_exc()}")
        finally:
            # UI'yi thread-safe ≈üekilde etkinle≈ütir
            self.root.after(0, self._reset_ui_prediction)
    
    def fetch_current_data(self):
        """G√ºncel veriyi √ßek ve g√∂ster"""
        if not self.city_var.get():
            messagebox.showerror("Hata", "L√ºtfen bir ≈üehir se√ßin!")
            return
            
        try:
            city = self.city_var.get()
            lat, lon = get_city_coordinates(city, self.airnow_params)
            
            self.log_message(f"üåê {city} i√ßin g√ºncel veri √ßekiliyor...")
            
            fetcher = RealTimeDataFetcher(airnow_params=self.airnow_params)
            current_data = fetcher.fetch_current_aqi(lat, lon)
            
            if current_data:
                self.log_message(f"‚úÖ {len(current_data)} parametre bulundu:")
                for item in current_data:
                    param_name = item.get('ParameterName', 'Unknown')
                    aqi_val = item.get('AQI', 0)
                    date_obs = item.get('DateObserved', 'Unknown')
                    hour_obs = item.get('HourObserved', 'Unknown')
                    category = self.get_aqi_category(aqi_val)
                    
                    self.log_message(f"   {param_name}: {aqi_val} AQI ({category})")
                    self.log_message(f"     Zaman: {date_obs} {hour_obs}:00")
            else:
                self.log_message("‚ùå G√ºncel veri bulunamadƒ±")
                
        except Exception as e:
            self.log_message(f"‚ùå Hata: {str(e)}")
    
    def get_aqi_category(self, aqi_value: float) -> str:
        """AQI deƒüerinden kategori √ßƒ±kar"""
        if aqi_value <= 50:
            return "ƒ∞yi"
        elif aqi_value <= 100:
            return "Orta"
        elif aqi_value <= 150:
            return "Hassas"
        elif aqi_value <= 200:
            return "Saƒülƒ±ksƒ±z"
        elif aqi_value <= 300:
            return "√áok Saƒülƒ±ksƒ±z"
        else:
            return "Tehlikeli"
    
    def run(self):
        """GUI'yi ba≈ülat"""
        self.root.mainloop()

def list_available_cities(airnow_params: Dict) -> List[str]:
    """
    Mevcut ≈üehirleri listeler.
    
    Args:
        airnow_params (Dict): airnow_viewer parametreleri
        
    Returns:
        List[str]: Mevcut ≈üehir listesi
    """
    if 'cities' not in airnow_params:
        return []
    return sorted(airnow_params['cities'].keys())

def auto_suggest_city(partial_name: str, airnow_params: Dict, limit: int = 5) -> List[str]:
    """
    Kƒ±smi ≈üehir adƒ±na g√∂re √∂neriler sunar.
    
    Args:
        partial_name (str): Kƒ±smi ≈üehir adƒ±
        airnow_params (Dict): airnow_viewer parametreleri
        limit (int): Maksimum √∂neri sayƒ±sƒ±
        
    Returns:
        List[str]: √ñnerilen ≈üehirler
    """
    if 'cities' not in airnow_params:
        return []
    
    partial_lower = partial_name.lower()
    suggestions = []
    
    for city in airnow_params['cities'].keys():
        if partial_lower in city.lower():
            suggestions.append(city)
            if len(suggestions) >= limit:
                break
    
    return suggestions

def create_demo_data() -> str:
    """
    Demo i√ßin √∂rnek AQI verisi olu≈üturur.
    
    Returns:
        str: Olu≈üturulan CSV dosyasƒ±nƒ±n yolu
    """
    import pandas as pd
    from datetime import datetime, timedelta
    import numpy as np
    
    # Demo verisi parametreleri
    demo_file = "demo_aqi_data.csv"
    cities = ["Denver, CO", "Los Angeles, CA", "New York, NY"]
    parameters = ["PM25", "O3"]
    start_date = datetime.now() - timedelta(days=30)
    
    # Veri olu≈üturma
    data = []
    for city in cities:
        city_parts = city.split(", ")
        reporting_area = city_parts[0]
        state_code = city_parts[1] if len(city_parts) > 1 else "XX"
        
        for param in parameters:
            for day in range(30):
                for hour in range(0, 24, 3):  # 3 saatte bir veri
                    current_time = start_date + timedelta(days=day, hours=hour)
                    
                    # Ger√ßek√ßi AQI deƒüerleri olu≈ütur
                    if param == "PM25":
                        base_aqi = 45 + np.random.normal(0, 15)
                    else:  # O3
                        base_aqi = 35 + np.random.normal(0, 12)
                    
                    # G√ºnl√ºk varyasyon ekle
                    hour_factor = 1 + 0.3 * np.sin(2 * np.pi * hour / 24)
                    aqi = max(0, int(base_aqi * hour_factor))
                    
                    data.append({
                        "ReportingArea": reporting_area,
                        "StateCode": state_code,
                        "ParameterName": param,
                        "AQI": aqi,
                        "DateObserved": current_time.strftime("%Y-%m-%d"),
                        "HourObserved": current_time.hour,
                        "Latitude": 39.7392 if "Denver" in city else (34.0522 if "Los Angeles" in city else 40.7128),
                        "Longitude": -104.9903 if "Denver" in city else (-118.2437 if "Los Angeles" in city else -74.0060)
                    })
    
    # DataFrame olu≈ütur ve kaydet
    df = pd.DataFrame(data)
    df.to_csv(demo_file, index=False)
    
    print(f"üìä Demo verisi olu≈üturuldu: {len(data)} satƒ±r, {len(cities)} ≈üehir, {len(parameters)} parametre")
    return demo_file

# --------------------------- Utilities ---------------------------------------



def safe_city_key(city: str) -> str:
    return " ".join(city.split()).strip().casefold()

def winsorize_series(x: pd.Series, p_low=0.01, p_high=0.99) -> pd.Series:
    if x.dropna().empty:
        return x
    lo, hi = x.quantile(p_low), x.quantile(p_high)
    return x.clip(lower=lo, upper=hi)

def rmse(y_true, y_pred):
    return sqrt(mean_squared_error(y_true, y_pred))

def mape(y_true, y_pred, eps=1e-6):
    y_true = np.asarray(y_true).astype(float)
    y_pred = np.asarray(y_pred).astype(float)
    mask = np.abs(y_true) > eps
    if mask.sum() == 0:
        return np.nan
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0

# --------------------------- Data Loading & Cleaning --------------------------

def build_ts(row):
    # Combine DateObserved (YYYY-MM-DD) and HourObserved (0-23) into timestamp
    d = str(row.get("DateObserved", "")).strip()
    try:
        h = int(row.get("HourObserved", 0))
    except Exception:
        h = 0
    try:
        return pd.Timestamp(d) + pd.Timedelta(hours=h)
    except Exception:
        return pd.NaT

def normalize_parameter(p):
    if pd.isna(p):
        return None
    return AQI_PARAM_MAP.get(str(p).strip(), str(p).strip())

def normalize_city_state(df: pd.DataFrame) -> pd.DataFrame:
    # ReportingArea s√ºtununu kontrol et ve d√ºzenle
    if "ReportingArea" in df.columns:
        df["ReportingArea"] = df["ReportingArea"].astype(str).str.strip()
    elif "City" in df.columns:
        df["ReportingArea"] = df["City"].astype(str).str.strip()
    else:
        df["ReportingArea"] = pd.Series([""]*len(df))
    
    # StateCode s√ºtununu kontrol et ve d√ºzenle
    if "StateCode" in df.columns:
        df["StateCode"] = df["StateCode"].astype(str).str.strip()
    else:
        df["StateCode"] = pd.Series([""]*len(df))
    
    df["CityKey"] = (df["ReportingArea"] + ", " + df["StateCode"]).str.strip()
    df["CityKeyNorm"] = df["CityKey"].apply(safe_city_key)
    return df

def load_and_clean(csv_path: str) -> pd.DataFrame:
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV not found: {csv_path}")

    df = pd.read_csv(csv_path)
    # Normalize columns
    if "ParameterName" in df.columns:
        df["Parameter"] = df["ParameterName"].apply(normalize_parameter)
    elif "Parameter" in df.columns:
        df["Parameter"] = df["Parameter"].apply(normalize_parameter)
    else:
        raise ValueError("CSV must contain 'ParameterName' or 'Parameter'.")

    if "AQI" not in df.columns:
        # some datasets use lowercase or different naming
        alt = [c for c in df.columns if c.lower() == "aqi"]
        if alt:
            df["AQI"] = df[alt[0]]
        else:
            raise ValueError("CSV must contain 'AQI' column.")

    # Build timestamp
    if "ts" not in df.columns:
        if ("DateObserved" in df.columns) and ("HourObserved" in df.columns):
            df["ts"] = df.apply(build_ts, axis=1)
        else:
            # try to parse any datetime-ish column
            dt_cols = [c for c in df.columns if "date" in c.lower() or "time" in c.lower() or c.lower()=="ts"]
            if not dt_cols:
                raise ValueError("CSV must contain either (DateObserved, HourObserved) or a datetime column (e.g., ts).")
            df["ts"] = pd.to_datetime(df[dt_cols[0]], errors="coerce")

    df = normalize_city_state(df)

    # Basic prune
    keep_cols = [
        "ReportingArea","StateCode","Latitude","Longitude",
        "Parameter","AQI","ts","temp_c","humidity"
    ]
    keep_cols = [c for c in keep_cols if c in df.columns]
    df = df[keep_cols].copy()

    # Drop bad timestamps
    df = df[pd.notna(df["ts"])].copy()
    # Dedup: mean AQI across duplicates
    g_cols = ["ReportingArea","StateCode","Parameter","ts"]
    df = df.groupby([c for c in g_cols if c in df.columns], as_index=False).agg({
        "AQI":"mean",
        **({"temp_c":"mean"} if "temp_c" in df.columns else {}),
        **({"humidity":"mean"} if "humidity" in df.columns else {}),
        **({"Latitude":"mean"} if "Latitude" in df.columns else {}),
        **({"Longitude":"mean"} if "Longitude" in df.columns else {}),
    })

    df = normalize_city_state(df)
    df.sort_values(["CityKey","Parameter","ts"], inplace=True)

    # Winsorize per series
    def _wins(g):
        g["AQI_raw"] = g["AQI"]
        g["AQI"] = winsorize_series(g["AQI"])
        return g
    df = df.groupby(["CityKey","Parameter"], group_keys=False).apply(_wins)

    return df

def reindex_hourly_and_ffill(df_city_param: pd.DataFrame) -> pd.DataFrame:
    # Assumes df filtered for a single (CityKey, Parameter)
    idx = pd.date_range(df_city_param["ts"].min(), df_city_param["ts"].max(), freq="H")
    df_city_param = df_city_param.set_index("ts").reindex(idx)
    df_city_param.index.name = "ts"
    # small gap ffill (‚â§3h) ‚Äî implement via limit=3
    param_col = "ParameterName" if "ParameterName" in df_city_param.columns else "Parameter"
    for col in ["AQI","temp_c","humidity","Latitude","Longitude","ReportingArea","StateCode","CityKey","CityKeyNorm",param_col]:
        if col in df_city_param.columns:
            if df_city_param[col].dtype.kind in "biufc":
                df_city_param[col] = df_city_param[col].ffill(limit=3)
            else:
                df_city_param[col] = df_city_param[col].ffill()
    df_city_param.reset_index(inplace=True)
    return df_city_param

# --------------------------- Feature Engineering -----------------------------

@dataclass
class ModelBundle:
    """Container for multi-horizon models"""
    models: Dict[int, object]  # horizon -> trained model
    train_timestamp: datetime
    city_encoders: Dict = None
    feature_names: List[str] = None
    interval_method: str = "none"
    conformal_residuals: Dict = None  # horizon -> calibration residuals

@dataclass
class FeatureConfig:
    lag_max: int = 168
    # Daily windows only; strictly shift(1) before roll to avoid leakage
    roll_windows: Tuple[int, ...] = (24, 48, 72, 96, 120, 144, 168)
    use_weather: bool = USE_EXOG
    use_cyclical: bool = True
    use_exog: bool = USE_EXOG
    use_quality_flags: bool = True
    
def get_aqi_risk_level(aqi_value: float) -> str:
    """Get AQI risk level for given value"""
    for level, (min_val, max_val) in AQI_THRESHOLDS.items():
        if min_val <= aqi_value <= max_val:
            return level
    return 'Hazardous'

def make_calendar_features(s: pd.Series) -> pd.DataFrame:
    df = pd.DataFrame(index=s.index.copy())
    dt = pd.to_datetime(s)
    df["hour"] = dt.dt.hour
    df["dow"] = dt.dt.dayofweek
    df["is_weekend"] = (df["dow"] >= 5).astype(int)
    # One-hot hour and dow (keep small dimensionality)
    hour_oh = pd.get_dummies(df["hour"], prefix="hr")
    dow_oh = pd.get_dummies(df["dow"], prefix="dow")
    return pd.concat([df[["is_weekend"]], hour_oh, dow_oh], axis=1)

def build_features(df: pd.DataFrame, city_meta: Dict = None, weather_df: pd.DataFrame = None, 
                  horizon: int = 1, is_training: bool = True) -> Tuple[pd.DataFrame, pd.Series, Dict]:
    """
    Enhanced feature engineering with exogenous features, quality flags, and cyclical encodings.
    STRICT ANTI-LEAKAGE: Uses only information available up to t (or t-1 for exogenous).
    
    Args:
        df: Input DataFrame with AQI time series
        city_meta: City metadata dict with lat/lon/climate_zone (optional)
        weather_df: External weather DataFrame (optional) 
        horizon: Forecast horizon (24-168 hours, daily steps)
        is_training: Whether building features for training or inference
        
    Returns:
        (X_df, y_series, aux_dict): Features, targets, auxiliary info
    """
    cfg = FeatureConfig()  # Use default config
    df = df.copy()
    
    # Strict anti-leakage: target is AQI shifted by -horizon
    df["y"] = df["AQI"].shift(-horizon)
    
    # === ENDOGENOUS FEATURES ===
    # AQI lags - daily patterns: 24h, 48h, 72h, 96h, 120h, 144h, 168h
    key_lags = [24, 48, 72, 96, 120, 144, 168]
    for lag in key_lags:
        if lag <= cfg.lag_max:
            df[f"aqi_lag_{lag}"] = df["AQI"].shift(lag)
    
    # Rolling statistics - use past values only with proper shift(1) to avoid leakage
    aqi_shifted = df["AQI"].shift(1)
    for w in cfg.roll_windows:
        if w <= cfg.lag_max:
            base = aqi_shifted.rolling(w, min_periods=max(6, w // 6))
            df[f"aqi_roll_mean_{w}"] = base.mean()
            df[f"aqi_roll_std_{w}"] = base.std()
            df[f"aqi_roll_min_{w}"] = base.min()
            df[f"aqi_roll_max_{w}"] = base.max()
    
    # === CYCLICAL TIME FEATURES ===
    if 'ts' in df.columns:
        dt = pd.to_datetime(df['ts'])
        # Hour of day (0-23) - cyclical encoding
        df['hour_sin'] = np.sin(2 * np.pi * dt.dt.hour / 24)
        df['hour_cos'] = np.cos(2 * np.pi * dt.dt.hour / 24)
        
        # Day of week (0-6) - cyclical encoding  
        df['dow_sin'] = np.sin(2 * np.pi * dt.dt.dayofweek / 7)
        df['dow_cos'] = np.cos(2 * np.pi * dt.dt.dayofweek / 7)
        
        # Month as categorical
        df['month'] = dt.dt.month
        
        # Weekend flag
        df['is_weekend'] = (dt.dt.dayofweek >= 5).astype(int)
        
        # Hour categories for rush hour, etc
        df['is_rush_hour'] = ((dt.dt.hour.isin([7, 8, 17, 18, 19]))).astype(int)
    
    # === EXOGENOUS FEATURES (if available) ===
    exog_features = []
    if USE_EXOG:
        # Temperature and humidity from current data - mirror daily lags
        for var in ["temp_c", "humidity", "wind_speed", "pressure"]:
            if var in df.columns:
                # Daily lag features to avoid leakage
                for lag in key_lags:
                    df[f"{var}_lag_{lag}"] = df[var].shift(lag)
                    exog_features.append(f"{var}_lag_{lag}")
        
        # External weather data (if provided)
        if weather_df is not None:
            # Left join up to t-1 to avoid leakage
            weather_lag = weather_df.copy()
            if 'ts' in weather_lag.columns:
                weather_lag['ts'] = pd.to_datetime(weather_lag['ts']) + pd.Timedelta(hours=1)
                df = df.merge(weather_lag[['ts', 'wind_speed', 'pressure']], on='ts', how='left')
                exog_features.extend(['wind_speed', 'pressure'])
        
        # External weather data (if provided)
        if weather_df is not None:
            # Left join up to t-1 to avoid leakage
            weather_lag = weather_df.copy()
            if 'ts' in weather_lag.columns:
                weather_lag['ts'] = pd.to_datetime(weather_lag['ts']) + pd.Timedelta(hours=1)
                df = df.merge(weather_lag[['ts', 'wind_speed', 'pressure']], on='ts', how='left')
                exog_features.extend(['wind_speed', 'pressure'])
    
    # === QUALITY FLAGS ===
    # Gap detection
    df['is_gap'] = df['AQI'].isna().astype(int)
    
    # Gap length (consecutive missing values)
    df['gap_len'] = df.groupby((~df['AQI'].isna()).cumsum())['AQI'].transform('size') * df['is_gap']
    
    # Data quality score (0-1) - based on recent data availability
    if 'ts' in df.columns:
        window_24h = df['AQI'].shift(1).rolling(24, min_periods=1)
        df['data_quality_score'] = 1.0 - window_24h.apply(lambda x: x.isna().mean())
    else:
        df['data_quality_score'] = 1.0
    
    # Holiday flag (stub for now - could be enhanced)
    df['is_holiday'] = 0
    
    # === CITY FEATURES (for global model) ===
    if city_meta:
        df['city_lat'] = city_meta.get('lat', 0)
        df['city_lon'] = city_meta.get('lon', 0) 
        df['climate_zone'] = city_meta.get('climate_zone', 'unknown')
    
    # === FEATURE SELECTION AND CLEANING ===
    # Get feature columns (exclude target, metadata, raw data)
    exclude_cols = {
        "y", "AQI", "AQI_raw", "ts", "DateObserved", "HourObserved",
        "ReportingArea", "StateCode", "CityKey", "CityKeyNorm", 
        "Parameter", "ParameterName", "Latitude", "Longitude"
    }
    
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    # Create auxiliary info DataFrame
    aux_cols = [col for col in ['ts', 'CityKey', 'ReportingArea', 'StateCode'] if col in df.columns]
    aux = df[aux_cols].copy() if aux_cols else pd.DataFrame()
    
    # Drop rows with NaN target or critical lags
    # Be more flexible - only require what's available
    available_lags = [f"aqi_lag_{lag}" for lag in [24, 48, 72] if f"aqi_lag_{lag}" in df.columns]
    dropna_cols = ["y"]
    
    # Only require lags that actually exist and have some data
    for lag_col in available_lags:
        if df[lag_col].notna().sum() > 0:  # Only require if there's some non-null data
            dropna_cols.append(lag_col)
    
    df_clean = df.dropna(subset=dropna_cols)
    
    if len(df_clean) == 0:
        # Try with just target variable
        df_clean = df.dropna(subset=["y"])
        if len(df_clean) == 0:
            raise ValueError(f"No valid data after dropping NaN values for horizon {horizon}")
        else:
            print(f"‚ö†Ô∏è Using reduced feature set for horizon {horizon}h due to insufficient data")
    
    # Extract features and target
    X = df_clean[feature_cols].select_dtypes(include=[np.number])
    y = df_clean["y"]
    aux_clean = aux.loc[df_clean.index] if not aux.empty else pd.DataFrame()
    
    # Winsorization - handle outliers at configurable percentiles
    for col in X.columns:
        if col.startswith(('aqi_', 'temp_', 'humidity')):  # Apply to main features
            col_data = X[col].dropna()
            if len(col_data) > 0:  # Only winsorize if we have data
                lower, upper = np.percentile(col_data, [WINSORIZE_LIMITS[0]*100, WINSORIZE_LIMITS[1]*100])
                X[col] = np.clip(X[col], lower, upper)
    
    # Also winsorize target
    if len(y.dropna()) > 10:  # Only if enough data
        y_lower, y_upper = np.percentile(y.dropna(), [WINSORIZE_LIMITS[0]*100, WINSORIZE_LIMITS[1]*100])
        y = np.clip(y, y_lower, y_upper)
    
    # Remove constant columns
    nunique = X.nunique()
    X = X.loc[:, nunique > 1]
    
    # Convert aux to dict format for contract compliance
    aux_dict = {}
    if not aux_clean.empty:
        aux_dict = {
            'timestamps': aux_clean.get('ts', pd.Series()),
            'city_keys': aux_clean.get('CityKey', pd.Series()),
            'metadata': aux_clean.to_dict('records') if len(aux_clean) > 0 else []
        }
    
    return X, y, aux_dict

# --------------------------- Modeling & Backtesting ---------------------------

def create_model(horizon: int, quantile: float = None, seed: int = RANDOM_STATE, 
                 with_hpo: bool = False, X_hpo: pd.DataFrame = None, y_hpo: pd.Series = None) -> object:
    """Create appropriate model based on available libraries and requirements"""
    
    if with_hpo and X_hpo is not None and y_hpo is not None and len(X_hpo) >= 200:
        # Hyperparameter optimization with time series aware splits
        if HAS_LIGHTGBM and lgb is not None and quantile is None:
            # LightGBM with HPO 
            param_dist = {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.05, 0.1, 0.15, 0.2],
                'max_depth': [3, 5, 7, 9],
                'num_leaves': [20, 31, 50],
                'subsample': [0.7, 0.8, 0.9],
                'colsample_bytree': [0.7, 0.8, 0.9, 1.0]
            }
            base_model = lgb.LGBMRegressor(random_state=seed, verbose=-1)
            
        elif quantile is not None and HAS_QUANTILE_GBDT:
            # HistGradientBoosting with HPO for quantiles
            param_dist = {
                'max_iter': [100, 200, 300],
                'learning_rate': [0.05, 0.1, 0.15],
                'max_depth': [3, 5, 7],
                'l2_regularization': [0.0, 0.1, 0.2]
            }
            base_model = HistGradientBoostingRegressor(
                loss='quantile', quantile=quantile, random_state=seed
            )
        else:
            # Sklearn GradientBoosting with HPO
            param_dist = {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.05, 0.1, 0.15],
                'max_depth': [3, 5, 7],
                'subsample': [0.7, 0.8, 0.9]
            }
            base_model = GradientBoostingRegressor(random_state=seed)
        
        # Time series aware cross-validation
        tscv = TimeSeriesSplit(n_splits=min(5, len(X_hpo) // 100), test_size=len(X_hpo) // 10)
        
        search = RandomizedSearchCV(
            base_model, param_dist, n_iter=20, cv=tscv, 
            scoring='neg_mean_absolute_error', random_state=seed, n_jobs=1
        )
        search.fit(X_hpo, y_hpo)
        return search.best_estimator_
    
    # Default models without HPO  
    if quantile is not None and HAS_QUANTILE_GBDT:
        # Use HistGradientBoostingRegressor for quantile regression
        return HistGradientBoostingRegressor(
            loss='quantile',
            quantile=quantile,
            max_iter=200,
            learning_rate=0.1,
            max_depth=6,
            random_state=seed
        )
    elif HAS_LIGHTGBM and lgb is not None and quantile is not None:
        # Use LightGBM for quantile regression
        return lgb.LGBMRegressor(
            objective='quantile',
            alpha=quantile,
            n_estimators=200,
            learning_rate=0.1,
            max_depth=6,
            random_state=seed,
            verbose=-1
        )
    elif HAS_LIGHTGBM and lgb is not None and quantile is None:
        # Use LightGBM for point predictions
        return lgb.LGBMRegressor(
            objective='regression',
            n_estimators=200,
            learning_rate=0.1,
            max_depth=6,
            random_state=seed,
            verbose=-1
        )
    else:
        # Fallback to standard GradientBoosting
        return GradientBoostingRegressor(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=6,
            subsample=0.8,
            random_state=seed
        )

def train_model(X: pd.DataFrame, y: pd.Series, horizon: int = 1, 
                quantile: float = None, with_hpo: bool = False) -> object:
    """Train a single model for given horizon and quantile"""
    
    min_samples = MIN_SAMPLES_PER_HORIZON.get(horizon, 50)
    if len(X) < min_samples:
        print(f"‚ö†Ô∏è Insufficient data for horizon {horizon}h: {len(X)} < {min_samples} samples")
        # Still train but with warning
    
    # Use HPO if enough data
    use_hpo = with_hpo and len(X) >= 200
    
    model = create_model(horizon, quantile, RANDOM_STATE, use_hpo, X, y)
    
    try:
        model.fit(X, y)
        
        # Store feature importances if available
        if hasattr(model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': X.columns,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            model._feature_importance_df = feature_importance  # Store for later use
        
        return model
    except Exception as e:
        print(f"‚ö†Ô∏è Model training failed for horizon {horizon}, quantile {quantile}: {e}")
        # Fallback to simple RandomForest
        from sklearn.ensemble import RandomForestRegressor
        model = RandomForestRegressor(
            n_estimators=100, 
            max_depth=10,
            random_state=RANDOM_STATE,
            n_jobs=1
        )
        model.fit(X, y)
        return model

def train_multi_horizon(train_df: pd.DataFrame, city_meta: Dict = None, weather_df: pd.DataFrame = None, 
                       horizons: List[int] = HORIZONS, with_hpo: bool = True) -> object:
    """
    Train global multi-horizon models with uncertainty quantification.
    
    Args:
        train_df: Combined DataFrame with all cities/data
        city_meta: City metadata for global model (optional)
        weather_df: External weather data (optional)
        horizons: List of forecast horizons 
        with_hpo: Enable hyperparameter optimization
        
    Returns:
        ModelBundle with trained models, feature names, and uncertainty artifacts
    """
    print(f"ü§ñ Training multi-horizon models for horizons: {horizons}")
    
    models = {}
    conformal_residuals = {}
    feature_names = None
    
    # Train models for each horizon
    for horizon in horizons:
        print(f"  üìà Training horizon {horizon}h...")
        
        try:
            # Build features for this horizon  
            X, y, aux = build_features(train_df, city_meta=city_meta, weather_df=weather_df, 
                                     horizon=horizon, is_training=True)
            
            if feature_names is None:
                feature_names = list(X.columns)
            
            # Split into train/validation for conformal calibration
            split_idx = int(len(X) * 0.8)
            X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
            y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
            
            if INTERVAL_METHOD == "quantile" and (HAS_QUANTILE_GBDT or (HAS_LIGHTGBM and lgb is not None)):
                # Train quantile models
                models[horizon] = {
                    'p10': train_model(X_train, y_train, horizon, quantile=0.1),
                    'p50': train_model(X_train, y_train, horizon, quantile=0.5), 
                    'p90': train_model(X_train, y_train, horizon, quantile=0.9)
                }
            else:
                # Train point model + conformal intervals
                model = train_model(X_train, y_train, horizon)
                models[horizon] = {'p50': model}
                
                # Compute conformal residuals on validation set
                if len(X_val) > 10:
                    y_pred = model.predict(X_val)
                    residuals = np.abs(y_val - y_pred)
                    conformal_residuals[horizon] = residuals
            
            print(f"    ‚úÖ Horizon {horizon}h: {len(X_train)} train, {len(X_val)} val samples")
            
        except Exception as e:
            print(f"    ‚ùå Failed to train horizon {horizon}h: {e}")
            continue
    
    return ModelBundle(
        models=models,
        train_timestamp=datetime.now(),
        feature_names=feature_names,
        interval_method=INTERVAL_METHOD,
        conformal_residuals=conformal_residuals
    )

def forecast_multi_horizon(context_df: pd.DataFrame, model_bundle: object, 
                          horizons: List[int] = HORIZONS) -> pd.DataFrame:
    """
    Generate multi-horizon forecasts with uncertainty intervals.
    
    Args:
        context_df: Current data context for making predictions
        model_bundle: Trained ModelBundle object
        horizons: Forecast horizons to generate
        
    Returns:
        DataFrame with columns: timestamp, horizon_h, y_pred, p10, p50, p90, prediction_mode
    """
    # Filter to available horizons in the bundle
    available_horizons = [h for h in horizons if h in model_bundle.models]
    
    results = []
    base_timestamp = pd.Timestamp.now() if 'ts' not in context_df.columns else context_df['ts'].max()
    
    for horizon in horizons:
        if horizon not in model_bundle.models:
            print(f"‚ö†Ô∏è No model for horizon {horizon}h, using fallback")
            # Fallback predictions
            last_aqi = context_df['AQI'].dropna().iloc[-1] if not context_df['AQI'].dropna().empty else 50
            results.append({
                'timestamp': base_timestamp + pd.Timedelta(hours=horizon),
                'horizon_h': horizon,
                'y_pred': last_aqi,
                'p10': last_aqi * 0.8,
                'p50': last_aqi,
                'p90': last_aqi * 1.2,
                'prediction_mode': 'fallback_carry'
            })
            continue
        
        try:
            # Build features for forecasting (no target needed)
            X_forecast, _, _ = build_features(context_df, horizon=horizon, is_training=False)
            
            if len(X_forecast) == 0:
                raise ValueError("No features available for forecasting")
            
            # Use last available row for forecasting
            X_last = X_forecast.iloc[[-1]].copy()  # Fix SettingWithCopyWarning
            
            # Align features with trained models
            if model_bundle.feature_names:
                missing_features = set(model_bundle.feature_names) - set(X_last.columns)
                for feat in missing_features:
                    X_last.loc[:, feat] = 0  # Fill missing features with 0 (use .loc to avoid warning)
                X_last = X_last[model_bundle.feature_names]
            
            models_h = model_bundle.models[horizon]
            prediction_mode = 'model'
            
            if isinstance(models_h, dict) and 'p50' in models_h:
                # Quantile models available
                p50 = models_h['p50'].predict(X_last)[0]
                
                if 'p10' in models_h and 'p90' in models_h:
                    p10 = models_h['p10'].predict(X_last)[0]
                    p90 = models_h['p90'].predict(X_last)[0]
                else:
                    # Use conformal intervals
                    if horizon in model_bundle.conformal_residuals:
                        residuals = model_bundle.conformal_residuals[horizon]
                        alpha = 0.2  # 80% coverage
                        q_low = np.quantile(residuals, alpha/2)
                        q_high = np.quantile(residuals, 1 - alpha/2)
                        p10 = p50 - q_high
                        p90 = p50 + q_high
                    else:
                        # Simple scaling fallback
                        p10 = p50 * 0.8
                        p90 = p50 * 1.2
                
                results.append({
                    'timestamp': base_timestamp + pd.Timedelta(hours=horizon),
                    'horizon_h': horizon,
                    'y_pred': p50,
                    'p10': max(0, p10),
                    'p50': max(0, p50),
                    'p90': min(500, p90),
                    'prediction_mode': prediction_mode
                })
            else:
                # Single point model
                pred = models_h.predict(X_last)[0]
                results.append({
                    'timestamp': base_timestamp + pd.Timedelta(hours=horizon),
                    'horizon_h': horizon,
                    'y_pred': pred,
                    'p10': max(0, pred * 0.8),
                    'p50': max(0, pred),
                    'p90': min(500, pred * 1.2),
                    'prediction_mode': prediction_mode
                })
                
        except Exception as e:
            print(f"‚ö†Ô∏è Forecast error for horizon {horizon}h: {e}")
            # Fallback to naive methods
            last_aqi = context_df['AQI'].dropna().iloc[-1] if not context_df['AQI'].dropna().empty else 50
            
            # Try seasonal naive (same hour last week) 
            if len(context_df) >= 24 * 7:
                seasonal_val = context_df['AQI'].iloc[-(24*7)]
                if not pd.isna(seasonal_val):
                    pred_val = seasonal_val
                    mode = 'fallback_seasonal'
                else:
                    pred_val = last_aqi
                    mode = 'fallback_carry'
            else:
                pred_val = last_aqi
                mode = 'fallback_carry'
            
            results.append({
                'timestamp': base_timestamp + pd.Timedelta(hours=horizon),
                'horizon_h': horizon,
                'y_pred': pred_val,
                'p10': pred_val * 0.8,
                'p50': pred_val,
                'p90': pred_val * 1.2,
                'prediction_mode': mode
            })
    
    forecast_df = pd.DataFrame(results)
    
    # Apply quantile crossing fix: ensure p10 <= p50 <= p90
    if 'p10' in forecast_df.columns and 'p90' in forecast_df.columns:
        forecast_df[['p10', 'p50', 'p90']] = forecast_df[['p10', 'p50', 'p90']].apply(
            lambda row: pd.Series(sorted([row['p10'], row['p50'], row['p90']])), axis=1
        )
    
    return forecast_df

# COVERAGE-CONTROL: Coverage adjustment for interval calibration
def adjust_intervals_for_coverage(forecast_df: pd.DataFrame, 
                                recent_coverage: Dict[int, float] = None) -> pd.DataFrame:
    """
    Adjust prediction intervals based on recent coverage performance.
    
    Args:
        forecast_df: Forecast DataFrame with p10, p50, p90 columns
        recent_coverage: Dict mapping horizon -> empirical coverage (0-1)
        
    Returns:
        DataFrame with adjusted intervals and adjustment metadata
    """
    if recent_coverage is None:
        return forecast_df  # No adjustment needed
    
    adjusted_df = forecast_df.copy()
    adjustments_made = {}
    
    for _, row in adjusted_df.iterrows():
        horizon = int(row['horizon_h'])
        
        if horizon in recent_coverage:
            empirical_coverage = recent_coverage[horizon]
            coverage_deviation = empirical_coverage - COVERAGE_TARGET
            
            # Adjust intervals if deviation exceeds tolerance
            if abs(coverage_deviation) > COVERAGE_TOLERANCE:
                if coverage_deviation < -COVERAGE_TOLERANCE:
                    # Under-covering: widen intervals
                    adjustment_factor = 1 + COVERAGE_ADJUSTMENT_FACTOR
                    adjustments_made[horizon] = f"Widened by {COVERAGE_ADJUSTMENT_FACTOR:.1%}"
                elif coverage_deviation > COVERAGE_TOLERANCE:
                    # Over-covering: narrow intervals (conservatively)
                    adjustment_factor = 1 - (COVERAGE_ADJUSTMENT_FACTOR * 0.5)
                    adjustments_made[horizon] = f"Narrowed by {COVERAGE_ADJUSTMENT_FACTOR*0.5:.1%}"
                else:
                    adjustment_factor = 1.0
                
                # Apply adjustment to intervals while preserving p50
                p50 = row['p50']
                p10_adj = p50 - (p50 - row['p10']) * adjustment_factor
                p90_adj = p50 + (row['p90'] - p50) * adjustment_factor
                
                # Update row
                adjusted_df.loc[adjusted_df['horizon_h'] == horizon, 'p10'] = max(0, p10_adj)
                adjusted_df.loc[adjusted_df['horizon_h'] == horizon, 'p90'] = min(500, p90_adj)
    
    # Store adjustment metadata for UI display
    if hasattr(adjusted_df, 'attrs'):
        adjusted_df.attrs['coverage_adjustments'] = adjustments_made
    
    return adjusted_df

# BLEND-LOCAL: Local model ensemble blending
def apply_local_blend(forecast_df: pd.DataFrame, 
                     context_df: pd.DataFrame,
                     city_name: str,
                     blend_local: bool = False) -> pd.DataFrame:
    """
    Blend global model predictions with local bias correction.
    
    Args:
        forecast_df: Global model forecasts
        context_df: Recent historical data for city
        city_name: Name of the city for local model
        blend_local: Whether to apply local blending
        
    Returns:
        DataFrame with blended predictions and blend metadata
    """
    if not blend_local or len(context_df) < MIN_LOCAL_SAMPLES:
        # Add metadata indicating no blending
        blended_df = forecast_df.copy()
        if hasattr(blended_df, 'attrs'):
            blended_df.attrs['blend_weights'] = {'global': 1.0, 'local': 0.0}
        return blended_df
    
    try:
        from sklearn.linear_model import Ridge
        
        blended_df = forecast_df.copy()
        blend_info = {}
        
        # Prepare local data for bias correction (simple approach)
        recent_data = context_df.tail(MIN_LOCAL_SAMPLES).copy()
        
        if len(recent_data) >= 24:  # Need minimum data for local model
            # Simple local bias model: predict recent trend
            recent_data['hour'] = pd.to_datetime(recent_data['ts']).dt.hour
            recent_data['lag_1'] = recent_data['AQI'].shift(1)
            recent_data = recent_data.dropna()
            
            if len(recent_data) >= 10:  # Minimum for Ridge
                # Fit local bias corrector with proper feature handling
                X_local = recent_data[['hour', 'lag_1']].fillna(0)
                y_local = recent_data['AQI']
                
                # Convert to numpy arrays to avoid feature name warnings
                X_local_array = X_local.values
                y_local_array = y_local.values
                
                local_model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
                local_model.fit(X_local_array, y_local_array)
                
                # Calculate local model validation score
                local_mae = np.mean(np.abs(local_model.predict(X_local_array) - y_local_array))
                
                # Dynamic blend weight based on local model performance
                # Better local performance = higher local weight
                base_global_weight = BLEND_GLOBAL_WEIGHT
                if local_mae < 20:  # Good local model
                    dynamic_local_weight = min(0.3, BLEND_LOCAL_WEIGHT * 1.5)
                elif local_mae > 50:  # Poor local model
                    dynamic_local_weight = max(0.05, BLEND_LOCAL_WEIGHT * 0.5)
                else:
                    dynamic_local_weight = BLEND_LOCAL_WEIGHT
                
                dynamic_global_weight = 1 - dynamic_local_weight
                
                # Apply blending to each forecast
                for idx, row in blended_df.iterrows():
                    # Generate local prediction (simplified)
                    last_hour = pd.to_datetime(context_df['ts']).iloc[-1].hour
                    last_aqi = context_df['AQI'].iloc[-1]
                    
                    # Use numpy array for prediction to avoid feature name warnings
                    local_X_array = np.array([[last_hour, last_aqi]])
                    local_pred = local_model.predict(local_X_array)[0]
                    
                    # Blend predictions
                    global_pred = row['y_pred']
                    blended_pred = (dynamic_global_weight * global_pred + 
                                  dynamic_local_weight * local_pred)
                    
                    # Update forecast
                    blended_df.loc[idx, 'y_pred'] = blended_pred
                    blended_df.loc[idx, 'p50'] = blended_pred
                    
                    # Adjust intervals proportionally
                    interval_adjustment = blended_pred / global_pred if global_pred > 0 else 1.0
                    blended_df.loc[idx, 'p10'] = max(0, row['p10'] * interval_adjustment)
                    blended_df.loc[idx, 'p90'] = min(500, row['p90'] * interval_adjustment)
                
                blend_info = {
                    'global': dynamic_global_weight,
                    'local': dynamic_local_weight,
                    'local_mae': local_mae
                }
            else:
                blend_info = {'global': 1.0, 'local': 0.0, 'reason': 'insufficient_local_data'}
        else:
            blend_info = {'global': 1.0, 'local': 0.0, 'reason': 'insufficient_recent_data'}
        
        # Store blend metadata
        if hasattr(blended_df, 'attrs'):
            blended_df.attrs['blend_weights'] = blend_info
        
        return blended_df
        
    except Exception as e:
        print(f"Local blending failed: {e}")
        return forecast_df

def naive_carry_forward(df: pd.DataFrame, horizons: List[int]) -> pd.DataFrame:
    """Naive carry-forward fallback"""
    last_aqi = df['AQI'].dropna().iloc[-1] if not df['AQI'].dropna().empty else 50
    results = []
    for h in horizons:
        results.append({
            'horizon_h': h,
            'y_pred': last_aqi,
            'p10': last_aqi * 0.9,
            'p50': last_aqi,
            'p90': last_aqi * 1.1,
            'prediction_mode': 'fallback_carry'
        })
    return pd.DataFrame(results)

def seasonal_naive(df: pd.DataFrame, horizons: List[int]) -> pd.DataFrame:
    """Seasonal naive fallback (same hour last week)"""
    results = []
    for h in horizons:
        # Try to get value from same hour last week
        lookback_idx = -(24 * 7 - h)
        if len(df) >= abs(lookback_idx) and lookback_idx < 0:
            seasonal_val = df['AQI'].iloc[lookback_idx]
            if not pd.isna(seasonal_val):
                pred_val = seasonal_val
                mode = 'fallback_seasonal'
            else:
                pred_val = df['AQI'].dropna().iloc[-1] if not df['AQI'].dropna().empty else 50
                mode = 'fallback_carry'
        else:
            pred_val = df['AQI'].dropna().iloc[-1] if not df['AQI'].dropna().empty else 50
            mode = 'fallback_carry'
        
        results.append({
            'horizon_h': h,
            'y_pred': pred_val,
            'p10': pred_val * 0.9,
            'p50': pred_val,
            'p90': pred_val * 1.1,
            'prediction_mode': mode
        })
    return pd.DataFrame(results)

def naive_baseline(y_hist: pd.Series, horizon: int) -> pd.Series:
    # y_hat(t+h) = y(t)
    return y_hist.shift(0)  # align later for test segments

def seasonal_baseline(y_hist: pd.Series, horizon: int) -> pd.Series:
    # y_hat(t+h) = y(t-24)
    return y_hist.shift(24 - horizon)  # will align through slicing

def rolling_backtest(df: pd.DataFrame, horizons: List[int] = None, n_folds: int = N_FOLDS, 
                     min_train_days: int = MIN_TRAIN_DAYS, step_hours: int = STEP_HOURS,
                     cfg: FeatureConfig = None, city_meta: Dict = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Leakage-free rolling-origin expanding window backtest.
    
    Args:
        df: Input data
        horizons: Forecast horizons to test
        n_folds: Number of backtest folds
        min_train_days: Minimum training days for first fold
        step_hours: Hours between fold start points
        cfg: Feature configuration
        city_meta: City metadata
        
    Returns:
        (summary_df, detail_df): Summary metrics and per-fold details
    """
    if horizons is None:
        horizons = HORIZONS
    if cfg is None:
        cfg = FeatureConfig()
    
    print(f"üîÑ Running rolling backtest: {n_folds} folds, horizons {horizons}")
    
    # Sort by timestamp
    df = df.sort_values('ts').reset_index(drop=True)
    
    # Calculate fold boundaries
    min_train_hours = min_train_days * 24
    total_hours = len(df)
    required_hours = min_train_hours + max(horizons) + (n_folds - 1) * step_hours
    
    if total_hours < required_hours:
        # Try to adjust parameters to fit available data
        if total_hours >= 200:  # Minimum viable backtest
            # Reduce requirements adaptively
            new_min_train_hours = min(min_train_hours, total_hours // 3)
            new_step_hours = min(step_hours, 6)
            new_n_folds = min(n_folds, max(2, (total_hours - new_min_train_hours - max(horizons)) // new_step_hours))
            
            if new_n_folds >= 2:
                print(f"‚ö†Ô∏è Adjusting backtest parameters to fit {total_hours} hours of data:")
                print(f"   Training: {new_min_train_hours // 24:.1f} days -> {new_min_train_hours} hours")
                print(f"   Step size: {step_hours}h -> {new_step_hours}h")
                print(f"   Folds: {n_folds} -> {new_n_folds}")
                
                min_train_hours = new_min_train_hours
                step_hours = new_step_hours
                n_folds = new_n_folds
            else:
                print(f"‚ùå Cannot adjust backtest parameters: need at least 2 viable folds")
                # Return empty results instead of raising exception
                return pd.DataFrame(), pd.DataFrame()
        else:
            print(f"‚ùå Insufficient data for backtesting: have {total_hours} hours, need >= 200 hours minimum")
            # Return empty results instead of raising exception
            return pd.DataFrame(), pd.DataFrame()
    
    print(f"üìã Final backtest config: {n_folds} folds, {min_train_hours}h training, {step_hours}h steps")
    
    detail_results = []
    
    for fold in range(n_folds):
        fold_start = min_train_hours + fold * step_hours
        test_start = fold_start + max(horizons)
        test_end = min(test_start + step_hours, total_hours)
        
        print(f"  üìä Fold {fold + 1}/{n_folds}: train=[0:{fold_start}], test=[{test_start}:{test_end}]")
        
        # Split data
        train_df = df.iloc[:fold_start].copy()
        test_df = df.iloc[test_start:test_end].copy()
        
        if len(train_df) < min_train_hours or len(test_df) < max(horizons):
            print(f"    ‚ö†Ô∏è Skipping fold {fold + 1}: insufficient data")
            continue
        
        # Test each horizon
        for horizon in horizons:
            try:
                # Build features for training
                X_train, y_train, _ = build_features(train_df, city_meta=city_meta, horizon=horizon, is_training=True)
                
                if len(X_train) < 50:
                    print(f"    ‚ö†Ô∏è Skipping horizon {horizon}h: insufficient features ({len(X_train)})")
                    continue
                
                # Train model for this horizon
                if INTERVAL_METHOD == "quantile":
                    # Train quantile models (P10, P50, P90) with HPO for larger datasets
                    use_hpo = len(X_train) >= 200
                    model_p50 = train_model(X_train, y_train, horizon=horizon, quantile=0.5, with_hpo=use_hpo)
                    model_p10 = train_model(X_train, y_train, horizon=horizon, quantile=0.1, with_hpo=use_hpo) 
                    model_p90 = train_model(X_train, y_train, horizon=horizon, quantile=0.9, with_hpo=use_hpo)
                    models = {'p10': model_p10, 'p50': model_p50, 'p90': model_p90}
                else:
                    # Standard point model with HPO if enough data
                    use_hpo = len(X_train) >= 200
                    model = train_model(X_train, y_train, horizon=horizon, with_hpo=use_hpo)
                    models = {'point': model}
                
                # Generate test predictions
                test_context = df.iloc[:test_start].copy()  # Use all data up to test point
                
                for test_idx in range(len(test_df)):
                    if test_idx + horizon >= len(test_df):
                        break
                    
                    # Current context for prediction (no future leakage)
                    current_context = test_context.copy()
                    if test_idx > 0:
                        # Add test data up to current point
                        current_context = pd.concat([
                            current_context,
                            test_df.iloc[:test_idx]
                        ], ignore_index=True)
                    
                    try:
                        # Build features for prediction
                        X_pred, _, _ = build_features(current_context, city_meta=city_meta, horizon=horizon, is_training=False)
                        
                        if len(X_pred) == 0:
                            continue
                        
                        # Predict using last available features
                        X_last = X_pred.iloc[[-1]]
                        
                        # Predict using models
                        if INTERVAL_METHOD == "quantile" and 'p50' in models:
                            # Quantile predictions
                            preds = {}
                            for q, model in models.items():
                                if hasattr(model, 'feature_names_in_'):
                                    missing_features = set(model.feature_names_in_) - set(X_last.columns)
                                    for feat in missing_features:
                                        X_last.loc[:, feat] = 0  # Fix SettingWithCopyWarning
                                    X_aligned = X_last[model.feature_names_in_]
                                else:
                                    X_aligned = X_last
                                preds[q] = model.predict(X_aligned)[0]
                            
                            pred = preds.get('p50', preds.get('point', 50))
                            p10_pred = preds.get('p10', pred * 0.8)
                            p90_pred = preds.get('p90', pred * 1.2)
                        else:
                            # Point prediction only
                            model = models.get('point', list(models.values())[0])
                            if hasattr(model, 'feature_names_in_'):
                                missing_features = set(model.feature_names_in_) - set(X_last.columns)
                                for feat in missing_features:
                                    X_last.loc[:, feat] = 0  # Fix SettingWithCopyWarning
                                X_last = X_last[model.feature_names_in_]
                            
                            pred = model.predict(X_last)[0]
                            p10_pred = pred * 0.8  # Rough intervals
                            p90_pred = pred * 1.2
                        
                        # Get true value
                        true_val = test_df.iloc[test_idx + horizon]['AQI']
                        
                        if pd.isna(true_val):
                            continue
                        
                        # Calculate baseline predictions
                        # Naive: carry forward current value
                        naive_pred = current_context['AQI'].iloc[-1] if not current_context['AQI'].empty else 50
                        
                        # Seasonal: same hour last week
                        if len(current_context) >= 24 * 7:
                            seasonal_pred = current_context['AQI'].iloc[-(24 * 7)]
                            if pd.isna(seasonal_pred):
                                seasonal_pred = naive_pred
                        else:
                            seasonal_pred = naive_pred
                        
                        # Store results with interval metrics
                        result_row = {
                            'fold': fold + 1,
                            'horizon_h': horizon,
                            'test_idx': test_idx,
                            'timestamp': test_df.iloc[test_idx + horizon]['ts'],
                            'y_true': true_val,
                            'y_pred_model': pred,
                            'y_pred_naive': naive_pred,
                            'y_pred_seasonal': seasonal_pred,
                            'ae_model': abs(true_val - pred),
                            'ae_naive': abs(true_val - naive_pred),
                            'ae_seasonal': abs(true_val - seasonal_pred),
                            'se_model': (true_val - pred) ** 2,
                            'se_naive': (true_val - naive_pred) ** 2,
                            'se_seasonal': (true_val - seasonal_pred) ** 2,
                            'ape_model': abs(true_val - pred) / max(abs(true_val), 1) * 100,
                            'ape_naive': abs(true_val - naive_pred) / max(abs(true_val), 1) * 100,
                            'ape_seasonal': abs(true_val - seasonal_pred) / max(abs(true_val), 1) * 100,
                        }
                        
                        # Add interval metrics if available
                        if INTERVAL_METHOD == "quantile":
                            result_row.update({
                                'p10_pred': p10_pred,
                                'p90_pred': p90_pred,
                                'coverage_80': 1 if p10_pred <= true_val <= p90_pred else 0,
                                'interval_width': p90_pred - p10_pred,
                                'pinball_loss_10': max(0.1 * (true_val - p10_pred), 0.9 * (p10_pred - true_val)),
                                'pinball_loss_90': max(0.9 * (true_val - p90_pred), 0.1 * (p90_pred - true_val))
                            })
                        
                        detail_results.append(result_row)
                        
                    except Exception as e:
                        print(f"      ‚ùå Prediction error at test_idx {test_idx}: {e}")
                        continue
                
                print(f"    ‚úÖ Horizon {horizon}h: {len([r for r in detail_results if r['fold'] == fold + 1 and r['horizon_h'] == horizon])} predictions")
                
            except Exception as e:
                print(f"    ‚ùå Horizon {horizon}h failed: {e}")
                continue
    
    # Create detail DataFrame
    detail_df = pd.DataFrame(detail_results)
    
    if detail_df.empty:
        print("‚ùå No backtest results generated")
        return pd.DataFrame(), pd.DataFrame()
    
    # Aggregate to summary statistics
    summary_rows = []
    
    for fold in detail_df['fold'].unique():
        for horizon in detail_df['horizon_h'].unique():
            fold_horizon_data = detail_df[(detail_df['fold'] == fold) & (detail_df['horizon_h'] == horizon)]
            
            if len(fold_horizon_data) == 0:
                continue
            
            summary_row = {
                'fold': fold,
                'horizon_h': horizon,
                'n_predictions': len(fold_horizon_data),
                'mae_model': fold_horizon_data['ae_model'].mean(),
                'rmse_model': np.sqrt(fold_horizon_data['se_model'].mean()),
                'mape_model': fold_horizon_data['ape_model'].mean(),
                'mae_naive': fold_horizon_data['ae_naive'].mean(),
                'rmse_naive': np.sqrt(fold_horizon_data['se_naive'].mean()),
                'mape_naive': fold_horizon_data['ape_naive'].mean(),
                'mae_seasonal': fold_horizon_data['ae_seasonal'].mean(),
                'rmse_seasonal': np.sqrt(fold_horizon_data['se_seasonal'].mean()),
                'mape_seasonal': fold_horizon_data['ape_seasonal'].mean(),
            }
            
            # Add interval metrics if available
            if INTERVAL_METHOD == "quantile" and 'coverage_80' in fold_horizon_data.columns:
                summary_row.update({
                    'coverage_80': fold_horizon_data['coverage_80'].mean(),
                    'mean_interval_width': fold_horizon_data['interval_width'].mean(),
                    'pinball_loss_10': fold_horizon_data['pinball_loss_10'].mean(),
                    'pinball_loss_90': fold_horizon_data['pinball_loss_90'].mean()
                })
            
            summary_rows.append(summary_row)
    
    summary_df = pd.DataFrame(summary_rows)
    
    # Print summary
    if not summary_df.empty:
        print("\nüìà Backtest Summary (Mean ¬± Std across folds):")
        print("=" * 80)
        
        for horizon in sorted(summary_df['horizon_h'].unique()):
            h_data = summary_df[summary_df['horizon_h'] == horizon]
            print(f"\nHorizon {horizon}h ({len(h_data)} folds):")
            print(f"  Model    - MAE: {h_data['mae_model'].mean():.2f}¬±{h_data['mae_model'].std():.2f}, "
                  f"RMSE: {h_data['rmse_model'].mean():.2f}¬±{h_data['rmse_model'].std():.2f}, "
                  f"MAPE: {h_data['mape_model'].mean():.1f}¬±{h_data['mape_model'].std():.1f}%")
            print(f"  Naive    - MAE: {h_data['mae_naive'].mean():.2f}¬±{h_data['mae_naive'].std():.2f}, "
                  f"RMSE: {h_data['rmse_naive'].mean():.2f}¬±{h_data['rmse_naive'].std():.2f}, "
                  f"MAPE: {h_data['mape_naive'].mean():.1f}¬±{h_data['mape_naive'].std():.1f}%")
            print(f"  Seasonal - MAE: {h_data['mae_seasonal'].mean():.2f}¬±{h_data['mae_seasonal'].std():.2f}, "
                  f"RMSE: {h_data['rmse_seasonal'].mean():.2f}¬±{h_data['rmse_seasonal'].std():.2f}, "
                  f"MAPE: {h_data['mape_seasonal'].mean():.1f}¬±{h_data['mape_seasonal'].std():.1f}%")
            
            # Show interval metrics if available
            if 'coverage_80' in h_data.columns:
                print(f"  Intervals - Coverage: {h_data['coverage_80'].mean():.1%}¬±{h_data['coverage_80'].std():.1%}, "
                      f"Width: {h_data['mean_interval_width'].mean():.1f}¬±{h_data['mean_interval_width'].std():.1f}")
                if h_data['coverage_80'].mean() < COVERAGE_THRESHOLD:
                    print(f"    ‚ö†Ô∏è Coverage below threshold ({COVERAGE_THRESHOLD:.0%}) - consider widening intervals")
    
    return summary_df, detail_df

def run_backtests(df: pd.DataFrame, horizons: List[int] = None, 
                  city_name: str = "Unknown") -> pd.DataFrame:
    """
    Convenience function to run backtests and return summary.
    """
    if horizons is None:
        horizons = HORIZONS[:4]  # Limit to first 4 horizons for speed
    
    print(f"üöÄ Running backtests for {city_name}")
    print(f"üìä Data: {len(df)} hours, horizons: {horizons}")
    
    try:
        summary_df, detail_df = rolling_backtest(df, horizons=horizons)
        return summary_df
    except Exception as e:
        print(f"‚ùå Backtest failed: {e}")
        return pd.DataFrame()

# --------------------------- Fallback Strategies & Error Analysis --------------

def naive_carry_forward(df: pd.DataFrame, horizon: int = 1) -> float:
    """Naive baseline: carry forward the most recent AQI value"""
    if df.empty or 'AQI' not in df.columns:
        return 50.0  # Default AQI if no data
    
    recent_values = df['AQI'].dropna()
    if recent_values.empty:
        return 50.0
    
    return float(recent_values.iloc[-1])

def seasonal_naive(df: pd.DataFrame, horizon: int = 1) -> float:
    """Seasonal baseline: same hour from 7 days ago"""
    if df.empty or 'AQI' not in df.columns or len(df) < 24 * 7:
        return naive_carry_forward(df, horizon)
    
    try:
        # Look for same hour 7 days ago
        seasonal_idx = len(df) - (24 * 7)
        if seasonal_idx >= 0 and seasonal_idx < len(df):
            seasonal_val = df.iloc[seasonal_idx]['AQI']
            if not pd.isna(seasonal_val):
                return float(seasonal_val)
    except:
        pass
    
    return naive_carry_forward(df, horizon)

def error_anatomy(detail_df: pd.DataFrame) -> pd.DataFrame:
    """Analyze errors by time patterns for regime-shift detection"""
    if detail_df.empty or 'timestamp' not in detail_df.columns:
        return pd.DataFrame()
    
    df = detail_df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['hour_of_day'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.dayofweek
    df['date'] = df['timestamp'].dt.date
    
    # Error by hour of day
    hourly_errors = df.groupby(['horizon_h', 'hour_of_day']).agg({
        'ae_model': ['mean', 'std', 'count']
    }).round(2)
    
    # Error by day of week  
    weekly_errors = df.groupby(['horizon_h', 'day_of_week']).agg({
        'ae_model': ['mean', 'std', 'count'] 
    }).round(2)
    
    # Recent vs historical error comparison (regime detection)
    regime_alerts = []
    for horizon in df['horizon_h'].unique():
        h_data = df[df['horizon_h'] == horizon].copy()
        if len(h_data) < 14:  # Need at least 2 weeks
            continue
            
        # Last 7 days vs historical
        recent_cutoff = h_data['timestamp'].max() - pd.Timedelta(days=7)
        recent_errors = h_data[h_data['timestamp'] >= recent_cutoff]['ae_model']
        historical_errors = h_data[h_data['timestamp'] < recent_cutoff]['ae_model']
        
        if len(recent_errors) > 0 and len(historical_errors) > 10:
            recent_mae = recent_errors.mean()
            historical_p90 = historical_errors.quantile(0.9)
            
            if recent_mae > historical_p90:
                regime_alerts.append({
                    'horizon_h': horizon,
                    'recent_mae': recent_mae,
                    'historical_p90': historical_p90,
                    'alert': 'potential_regime_shift'
                })
    
    regime_df = pd.DataFrame(regime_alerts)
    
    return {
        'hourly_errors': hourly_errors,
        'weekly_errors': weekly_errors, 
        'regime_alerts': regime_df
    }

# --------------------------- Conformal Intervals ------------------------------

def conformal_interval(residuals: np.ndarray, alpha: float = 0.1) -> float:
    """
    Simple absolute residual quantile for two-sided symmetric PI.
    """
    residuals = np.abs(np.asarray(residuals))
    q = np.quantile(residuals, 1 - alpha)
    return float(q)

# --------------------------- Forecasting --------------------------------------

def iterative_forecast(full_df: pd.DataFrame, model, horizon: int, cfg: FeatureConfig, alpha: float = 0.1) -> pd.DataFrame:
    """
    Produce next `horizon` hourly forecasts iteratively (1-step ahead repeated).
    If exogenous features are present but not available for the future, we persist last values.
    """
    df = full_df.copy()
    last_ts = df["ts"].max()
    # Prepare residuals for conformal width: use a small validation split
    X_all, y_all, _ = build_features(df, horizon=horizon, is_training=True)
    if len(X_all) < 100:
        conf_w = np.nan
    else:
        val_cut = int(len(X_all) * 0.8)
        try:
            y_hat_val = model.predict(X_all.iloc[val_cut:])
            res = y_all.iloc[val_cut:] - y_hat_val
            conf_w = conformal_interval(res.values, alpha=alpha)
        except NotFittedError:
            conf_w = np.nan

    # Iterate hour by hour
    preds = []
    exo_cols = [c for c in ["temp_c","humidity"] if c in df.columns]
    last_exo = {c: df[c].dropna().iloc[-1] if c in df.columns else None for c in exo_cols}

    for step in range(1, horizon + 1):
        future_ts = last_ts + pd.Timedelta(hours=step)
        # append a placeholder row for feature computation
        row = {
            "ts": future_ts,
            "AQI": np.nan,
            "AQI_raw": np.nan,
        }
        for c in ["ReportingArea","StateCode","CityKey","CityKeyNorm","Parameter","Latitude","Longitude"]:
            if c in df.columns:
                row[c] = df[c].iloc[-1]
        for ex in exo_cols:
            row[ex] = last_exo.get(ex)
        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)

        Xf, _, _ = build_features(df, horizon=horizon, is_training=False)
        # last row of Xf corresponds to future target time
        x_next = Xf.iloc[[-1]]
        yhat = float(model.predict(x_next)[0])
        preds.append((future_ts, yhat))

        # write back predicted AQI at its "observed time" for recursive features:
        df.loc[df["ts"] == future_ts, "AQI"] = yhat
        df.loc[df["ts"] == future_ts, "AQI_raw"] = yhat

    fc = pd.DataFrame(preds, columns=["ts","yhat"]).set_index("ts")
    if not np.isnan(conf_w):
        fc["pi_lo"] = fc["yhat"] - conf_w
        fc["pi_hi"] = fc["yhat"] + conf_w
    else:
        fc["pi_lo"] = np.nan
        fc["pi_hi"] = np.nan
    return fc.reset_index()

# --------------------------- Orchestration ------------------------------------

def select_city_series(df: pd.DataFrame, city_str: str, parameter: str) -> pd.DataFrame:
    parameter = AQI_PARAM_MAP.get(parameter, parameter)
    # API'den gelen veri ParameterName s√ºtunu kullanƒ±yor
    param_col = "ParameterName" if "ParameterName" in df.columns else "Parameter"
    df = df[df[param_col] == parameter].copy()
    target_key = safe_city_key(city_str)
    # Exact match preferred
    matches = df[df["CityKeyNorm"] == target_key].copy()
    if matches.empty:
        # try to relax (starts with city name ignoring state)
        base_city = safe_city_key(city_str.split(",")[0])
        matches = df[df["CityKeyNorm"].str.startswith(base_city)].copy()
    if matches.empty:
        available = df["CityKey"].dropna().unique()[:30]
        raise ValueError(f"No data for '{city_str}' & '{parameter}'. Available sample keys: {list(available)}")
    # Reindex hourly + ffill small gaps
    matches = reindex_hourly_and_ffill(matches)
    # Ensure enough history (ge√ßici olarak azaltƒ±ldƒ± test i√ßin)
    valid = matches["AQI"].dropna()
    if len(valid) < 30:
        raise ValueError(f"Insufficient history for {city_str}-{parameter}: only {len(valid)} valid hourly points (need ‚â•30).")
    return matches

def add_features(series: pd.DataFrame) -> pd.DataFrame:
    """Feature engineering i√ßin √∂zellikleri ekle"""
    df = series.copy()
    
    # Temizlik
    df["AQI"] = pd.to_numeric(df["AQI"], errors="coerce")
    df = df.dropna(subset=["AQI"])
    
    if len(df) < 30:
        raise ValueError("Yeterli veri yok - en az 30 nokta gerekli")
    
    # Lag features - daily patterns
    for lag in [24, 48, 72, 96]:
        df[f"aqi_lag_{lag}"] = df["AQI"].shift(lag)
    
    # Rolling statistics - daily windows
    for window in [24, 48, 72]:
        df[f"aqi_roll_mean_{window}"] = df["AQI"].rolling(window).mean()
        df[f"aqi_roll_std_{window}"] = df["AQI"].rolling(window).std()
    
    # Calendar features
    if 'ts' in df.columns:
        # ts s√ºtunu datetime formatƒ±nda olmalƒ±
        df['ts'] = pd.to_datetime(df['ts'])
        df['hour'] = df['ts'].dt.hour
        df['day_of_week'] = df['ts'].dt.dayofweek
        df['month'] = df['ts'].dt.month
    elif 'datetime' in df.columns:
        df['hour'] = pd.to_datetime(df['datetime']).dt.hour
        df['day_of_week'] = pd.to_datetime(df['datetime']).dt.dayofweek
        df['month'] = pd.to_datetime(df['datetime']).dt.month
    else:
        df['hour'] = df.index.hour if hasattr(df.index, 'hour') else 12
        df['day_of_week'] = df.index.dayofweek if hasattr(df.index, 'dayofweek') else 0
        df['month'] = df.index.month if hasattr(df.index, 'month') else 1
    
    # Eksik deƒüerleri doldur
    df = df.fillna(method='ffill').fillna(method='bfill')
    
    return df

def fit_model(df: pd.DataFrame) -> Tuple[GradientBoostingRegressor, Dict]:
    """Model eƒüit ve performans bilgilerini d√∂nd√ºr"""
    
    # Feature ve target ayƒ±r
    feature_cols = [col for col in df.columns if col not in ['AQI', 'datetime', 'Parameter', 'ParameterName', 'CityKey', 'CityKeyNorm', 'ts', 'DateObserved', 'HourObserved', 'ReportingArea', 'StateCode', 'Latitude', 'Longitude']]
    
    if not feature_cols:
        # Basit lag feature'lar ekle
        df['aqi_lag_1'] = df['AQI'].shift(1)
        df['aqi_lag_24'] = df['AQI'].shift(24)
        feature_cols = ['aqi_lag_1', 'aqi_lag_24']
    
    # Eksik deƒüerleri temizle
    df_clean = df.dropna(subset=['AQI'] + feature_cols)
    
    if len(df_clean) < 30:
        raise ValueError("Model eƒüitimi i√ßin yeterli veri yok")
    
    X = df_clean[feature_cols]
    y = df_clean['AQI']
    
    # Model eƒüit
    model = GradientBoostingRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=6,
        random_state=42
    )
    
    # Train/test split (son %20 test i√ßin)
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    
    model.fit(X_train, y_train)
    
    # Performans hesapla
    if len(X_test) > 0:
        y_pred = model.predict(X_test)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = sqrt(mean_squared_error(y_test, y_pred))
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    else:
        mae = rmse = mape = 0
    
    performance = {
        'mae': mae,
        'rmse': rmse,
        'mape': mape
    }
    
    return model, performance

def forecast_future(model: GradientBoostingRegressor, df: pd.DataFrame, horizon: int) -> List[float]:
    """Gelecek tahminleri olu≈ütur - fit_model ile aynƒ± feature s√ºtunlarƒ±nƒ± kullan"""
    
    # fit_model ile aynƒ± feature filtreleme mantƒ±ƒüƒ±nƒ± kullan
    feature_cols = [col for col in df.columns if col not in ['AQI', 'datetime', 'Parameter', 'ParameterName', 'CityKey', 'CityKeyNorm', 'ts', 'DateObserved', 'HourObserved', 'ReportingArea', 'StateCode', 'Latitude', 'Longitude']]
    
    # Son temiz veriyi al
    df_clean = df.dropna(subset=['AQI']).copy()
    
    if len(df_clean) == 0:
        # Varsayƒ±lan tahmin
        last_aqi = df['AQI'].dropna().iloc[-1] if not df['AQI'].dropna().empty else 50
        return [last_aqi] * horizon
    
    forecast = []
    
    for step in range(horizon):
        # Son veri noktasƒ± √ºzerinden dinamik feature'lar olu≈ütur
        if len(df_clean) >= 24:
            # Lag features
            lag_features = {
                'aqi_lag_1': df_clean['AQI'].iloc[-1] if len(df_clean) >= 1 else 50,
                'aqi_lag_6': df_clean['AQI'].iloc[-6] if len(df_clean) >= 6 else 50,
                'aqi_lag_12': df_clean['AQI'].iloc[-12] if len(df_clean) >= 12 else 50,
                'aqi_lag_24': df_clean['AQI'].iloc[-24] if len(df_clean) >= 24 else 50,
            }
            
            # Rolling features
            recent_values = df_clean['AQI'].tail(24)
            rolling_features = {
                'aqi_roll_mean_6': recent_values.tail(6).mean(),
                'aqi_roll_std_6': recent_values.tail(6).std(),
                'aqi_roll_mean_12': recent_values.tail(12).mean(),
                'aqi_roll_std_12': recent_values.tail(12).std(),
                'aqi_roll_mean_24': recent_values.mean(),
                'aqi_roll_std_24': recent_values.std(),
            }
            
            # Calendar features (sonraki saati tahmin et)
            if 'ts' in df_clean.columns:
                last_ts = pd.to_datetime(df_clean['ts'].iloc[-1])
                next_ts = last_ts + pd.Timedelta(hours=step+1)
                calendar_features = {
                    'hour': next_ts.hour,
                    'day_of_week': next_ts.dayofweek,
                    'month': next_ts.month,
                }
            else:
                calendar_features = {
                    'hour': 12,
                    'day_of_week': 0,
                    'month': 1,
                }
            
            # T√ºm feature'larƒ± birle≈ütir
            features = {**lag_features, **rolling_features, **calendar_features}
            
        else:
            # Yeterli veri yoksa basit tahmin
            last_aqi = df_clean['AQI'].iloc[-1]
            features = {col: 0 for col in feature_cols}
            if 'aqi_lag_1' in features:
                features['aqi_lag_1'] = last_aqi
        
        # Model feature'larƒ±nda olmayan s√ºtunlarƒ± filtrele
        model_features = model.feature_names_in_ if hasattr(model, 'feature_names_in_') else feature_cols
        filtered_features = {col: features.get(col, 0) for col in model_features}
        
        # Tahmin yap
        try:
            X_pred = pd.DataFrame([filtered_features])
            pred = model.predict(X_pred)[0]
        except Exception as e:
            print(f"‚ö†Ô∏è Tahmin hatasƒ±: {e}")
            pred = df_clean['AQI'].iloc[-1]  # Son deƒüeri kullan
        
        # Makul sƒ±nƒ±rlar
        pred = max(0, min(500, pred))
        forecast.append(pred)
        
        # Sonraki tahmin i√ßin veriyi g√ºncelle
        new_row = df_clean.iloc[-1].copy()
        new_row['AQI'] = pred
        if 'ts' in new_row.index:
            new_row['ts'] = pd.to_datetime(new_row['ts']) + pd.Timedelta(hours=1)
        
        # DataFrame'e yeni satƒ±r ekle
        df_clean = pd.concat([df_clean, new_row.to_frame().T], ignore_index=True)
    
    return forecast

def ensure_dirs(out_dir: str, city_key: str, parameter: str) -> str:
    city_dir = os.path.join(out_dir, city_key.replace("/", "_"), parameter)
    os.makedirs(city_dir, exist_ok=True)
    return city_dir

def run_pipeline(args):
    """Main pipeline with multi-horizon forecasting support."""
    # √ñzel komutlarƒ± kontrol et
    if args.list_cities:
        print("Mevcut ≈üehirler:")
        cities = list_available_cities(args.airnow_params)
        if cities:
            for i, city in enumerate(cities, 1):
                print(f"{i:3d}. {city}")
            print(f"\nToplam: {len(cities)} ≈üehir")
        else:
            print("airnow_viewer parametreleri y√ºklenemedi.")
        return
    
    if args.suggest_city:
        print(f"'{args.suggest_city}' i√ßin √∂neriler:")
        suggestions = auto_suggest_city(args.suggest_city, args.airnow_params)
        if suggestions:
            for i, city in enumerate(suggestions, 1):
                print(f"{i}. {city}")
        else:
            print("√ñneri bulunamadƒ±.")
        return
    
    # ≈ûehir koordinatlarƒ±nƒ± otomatik √ßek
    if args.auto_coords and args.airnow_params:
        try:
            lat, lon = get_city_coordinates(args.city, args.airnow_params)
            if args.verbose:
                print(f"‚úÖ '{args.city}' i√ßin koordinatlar: ({lat}, {lon})")
        except ValueError as e:
            print(f"‚ö†Ô∏è ≈ûehir koordinatlarƒ± alƒ±namadƒ±: {e}")
            # √ñnerileri g√∂ster
            suggestions = auto_suggest_city(args.city, args.airnow_params, limit=3)
            if suggestions:
                print(f"√ñneriler: {suggestions}")
    
    try:
        # YENI: Ger√ßek zamanlƒ± veri √ßekme modu
        if args.real_time:
            print("üåê Ger√ßek zamanlƒ± AQI verisi √ßekiliyor...")
            
            # Koordinatlarƒ± al
            lat, lon = get_city_coordinates(args.city, args.airnow_params)
            
            # Ger√ßek veri √ßek
            fetcher = RealTimeDataFetcher(airnow_params=args.airnow_params)
            df = fetcher.create_historical_data(lat, lon, args.city, days=30)
            
            if args.verbose:
                print(f"üìä Ger√ßek API verisi: {len(df)} satƒ±r")
            
            # API verisini normalize et (load_and_clean ile aynƒ± i≈ülemi yap)
            if "ParameterName" in df.columns:
                df["Parameter"] = df["ParameterName"]
            
            # Timestamp olu≈ütur
            if "ts" not in df.columns and "DateObserved" in df.columns and "HourObserved" in df.columns:
                df["ts"] = df.apply(build_ts, axis=1)
            
            # City key olu≈ütur  
            df = normalize_city_state(df)
        else:
            # Mevcut CSV dosyasƒ± varsa kullan
            if args.data_csv and os.path.exists(args.data_csv):
                df = load_and_clean(args.data_csv)
                if args.verbose:
                    print(f"üìä CSV'den y√ºklenen veri: {len(df)} satƒ±r, {len(df.columns)} s√ºtun")
            else:
                # CSV yoksa ger√ßek veri √ßek
                print("üìÅ CSV dosyasƒ± bulunamadƒ±, ger√ßek zamanlƒ± veri √ßekiliyor...")
                lat, lon = get_city_coordinates(args.city, args.airnow_params)
                fetcher = RealTimeDataFetcher(airnow_params=args.airnow_params)
                df = fetcher.create_historical_data(lat, lon, args.city, days=30)
        
        if args.verbose:
            print(f"üèôÔ∏è Mevcut ≈üehirler: {df['ReportingArea'].unique() if 'ReportingArea' in df.columns else '≈ûehir bilgisi yok'}")
            print(f"üî¨ Parametreler: {df['ParameterName'].unique() if 'ParameterName' in df.columns else 'Parametre bilgisi yok'}")
        
        series = select_city_series(df, args.city, args.parameter)
        if args.verbose:
            print(f"‚úÖ ≈ûehir verisi se√ßildi: {len(series)} satƒ±r")
    except Exception as e:
        print(f"‚ùå Veri y√ºkleme/se√ßme hatasƒ±: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return

    cfg = FeatureConfig()
    city_key = series["CityKey"].dropna().iloc[-1]
    city_dir = ensure_dirs(args.out_dir, city_key, AQI_PARAM_MAP.get(args.parameter, args.parameter))

    # Koordinat bilgisini kaydet
    if args.auto_coords and args.airnow_params:
        try:
            lat, lon = get_city_coordinates(args.city, args.airnow_params)
            coord_info = {
                "city_name": args.city,
                "coordinates": {"latitude": lat, "longitude": lon},
                "source": "airnow_viewer.py",
                "timestamp": datetime.now().isoformat()
            }
            with open(os.path.join(city_dir, "coordinates.json"), "w", encoding="utf-8") as f:
                json.dump(coord_info, f, ensure_ascii=False, indent=2)
            if args.verbose:
                print(f"üìç Koordinat bilgisi kaydedildi: {os.path.join(city_dir, 'coordinates.json')}")
        except Exception as e:
            if args.verbose:
                print(f"‚ö†Ô∏è Koordinat bilgisi kaydedilemedi: {e}")

    # Enhanced multi-horizon system
    city_meta = {}
    if args.airnow_params:
        try:
            lat, lon = get_city_coordinates(args.city, args.airnow_params)
            city_meta = {"lat": lat, "lon": lon, "name": args.city}
        except:
            pass

    # Backtest with multi-horizon
    if args.mode in ("train", "train_and_forecast"):
        print("\nüîÑ Running enhanced multi-horizon backtests...")
        
        # Use specified horizon or default horizons
        test_horizons = [args.horizon] if hasattr(args, 'horizon') and args.horizon else HORIZONS[:3]
        
        try:
            bt_summary, bt_detail = rolling_backtest(
                series, 
                horizons=test_horizons, 
                city_meta=city_meta
            )
            
            # Save backtest results
            if not bt_summary.empty:
                bt_summary.to_csv(os.path.join(city_dir, "backtest_summary.csv"), index=False)
                print(f"üìä Backtest summary saved: {os.path.join(city_dir, 'backtest_summary.csv')}")
            
            if not bt_detail.empty:
                bt_detail.to_csv(os.path.join(city_dir, "backtest_detail.csv"), index=False)
                print(f"üìä Backtest details saved: {os.path.join(city_dir, 'backtest_detail.csv')}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Backtest failed: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()

    # Train multi-horizon models
    if args.mode in ("train", "train_and_forecast"):
        print("\nü§ñ Training multi-horizon models...")
        
        try:
            bundle = train_multi_horizon(
                series, 
                horizons=HORIZONS, 
                city_meta=city_meta
            )
            
            # Save model bundle
            bundle_path = os.path.join(city_dir, "model_bundle.pkl")
            joblib.dump(bundle, bundle_path)
            print(f"üíæ Model bundle saved: {bundle_path}")
            
            # Save model info
            model_info = {
                "horizons": list(bundle.models.keys()),
                "trained_at": datetime.now().isoformat(),
                "data_shape": [int(len(series)), int(len(series.columns))],
                "time_span": [str(series["ts"].min()), str(series["ts"].max())],
                "parameter": AQI_PARAM_MAP.get(args.parameter, args.parameter),
                "city_key": city_key,
                "city_meta": city_meta,
                "model_count": len(bundle.models),
                "interval_method": INTERVAL_METHOD,
                "feature_config": {
                    "use_weather": cfg.use_weather,
                    "use_cyclical": cfg.use_cyclical,
                    "use_exog": USE_EXOG
                }
            }
            
            if args.airnow_params:
                model_info["airnow_integration"] = {
                    "available_cities_count": len(args.airnow_params.get('cities', {})),
                    "api_endpoint": args.airnow_params.get('base_url', 'unknown'),
                    "coordinates_source": "airnow_viewer.py"
                }
            
            with open(os.path.join(city_dir, "model_info.json"), "w", encoding="utf-8") as f:
                json.dump(model_info, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"‚ùå Model training failed: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
            return

    # Single-horizon forecast (respect user's exact choice)
    if args.mode in ("forecast", "train_and_forecast"):
        print(f"\nüîÆ Generating forecast for EXACT horizon: {args.horizon}h...")
        
        try:
            # Load or use existing bundle
            bundle_path = os.path.join(city_dir, "model_bundle.pkl")
            if os.path.exists(bundle_path):
                bundle = joblib.load(bundle_path)
                print(f"üìÅ Loaded existing model bundle: {len(bundle.models)} models")
            else:
                print("‚ùå No model bundle found - train models first")
                return
            
            # Generate forecast for EXACT selected horizon only
            forecasts = forecast_multi_horizon(
                series, 
                bundle,
                horizons=[args.horizon]  # Use only the user's selected horizon
            )
            
            # Save forecasts
            forecast_path = os.path.join(city_dir, "forecasts_multi_horizon.csv")
            forecasts.to_csv(forecast_path, index=False)
            print(f"üîÆ Multi-horizon forecasts saved: {forecast_path}")
            
            # Print forecast summary
            print("\nüìà Forecast Summary:")
            print("=" * 60)
            for horizon in sorted(forecasts['horizon_h'].unique()):
                h_data = forecasts[forecasts['horizon_h'] == horizon]
                if not h_data.empty:
                    pred = h_data['pred_mean'].iloc[0]
                    lower = h_data['pred_lower'].iloc[0] if 'pred_lower' in h_data.columns else None
                    upper = h_data['pred_upper'].iloc[0] if 'pred_upper' in h_data.columns else None
                    
                    print(f"Horizon {horizon:2d}h: {pred:6.1f}", end="")
                    if lower is not None and upper is not None:
                        print(f" [{lower:5.1f}, {upper:5.1f}]", end="")
                    
                    # Add AQI category
                    category = "Good" if pred <= 50 else "Moderate" if pred <= 100 else "Unhealthy" if pred <= 150 else "Very Unhealthy"
                    print(f" ({category})")
            
        except Exception as e:
            print(f"‚ùå Forecasting failed: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()

    print(f"\nüìÅ All artifacts saved in: {city_dir}")
    
    # airnow_viewer entegrasyonu hakkƒ±nda √∂zet
    if args.airnow_params and args.verbose:
        print(f"\nüåç airnow_viewer.py entegrasyonu:")
        print(f"   üìç {len(args.airnow_params.get('cities', {}))} ≈üehir koordinatlarƒ± y√ºklendi")
        print(f"   üîó API endpoint: {args.airnow_params.get('base_url', 'N/A')}")
        if args.auto_coords:
            try:
                lat, lon = get_city_coordinates(args.city, args.airnow_params)
                print(f"   üìå Kullanƒ±lan koordinatlar: ({lat}, {lon})")
            except:
                pass

# --------------------------- CLI ---------------------------------------------

def parse_args():
    # airnow_viewer parametrelerini y√ºkle
    airnow_params = import_airnow_params()
    if airnow_params:
        print(f"‚úÖ airnow_viewer.py'den {len(airnow_params.get('cities', {}))} ≈üehir parametresi ba≈üarƒ±yla y√ºklendi.")
    else:
        print("‚ö†Ô∏è airnow_viewer.py parametreleri y√ºklenemedi!")
    
    p = argparse.ArgumentParser(description="üåç AeroSafe - AQI Tahmin Sistemi")
    
    # Ana modlar
    p.add_argument("--gui", action="store_true", help="üñ•Ô∏è GUI modunda √ßalƒ±≈ütƒ±r")
    p.add_argument("--real-time", action="store_true", help="üåê Ger√ßek zamanlƒ± API verisi kullan")
    
    # Yeni √∂zellikler
    p.add_argument("--list-cities", action="store_true", help="Mevcut ≈üehirleri listele")
    p.add_argument("--suggest-city", type=str, metavar="NAME", help="≈ûehir √∂nerileri al")
    p.add_argument("--verbose", action="store_true", help="Detaylƒ± √ßƒ±ktƒ±")
    p.add_argument("--auto-coords", action="store_true", default=True, help="Otomatik koordinat √ßekme")
    
    # Ana parametreler - GUI modunda isteƒüe baƒülƒ±
    p.add_argument("--data_csv", help="CSV dosyasƒ± yolu")
    p.add_argument("--city", help="≈ûehir adƒ± (√∂rn: Denver, CO)")
    p.add_argument("--parameter", choices=["PM25","O3","PM2.5","pm25","o3"], help="Hava kirletici t√ºr√º")
    p.add_argument("--horizon", type=int, default=24,
                   choices=[24, 48, 72, 96, 120, 144, 168],
                   help="Forecast horizon (hours; daily steps 24..168)")
    p.add_argument("--mode", default="train_and_forecast", choices=["train","forecast","train_and_forecast"])
    p.add_argument("--out_dir", default="./artifacts")
    p.add_argument("--seed", type=int, default=42)
    p.add_argument("--n_jobs", type=int, default=-1)
    p.add_argument("--alpha", type=float, default=0.1, help="Conformal alpha")
    
    # BLEND-LOCAL: New ensemble blending flag
    p.add_argument("--blend_local", action="store_true", default=False, 
                  help="Enable local-global ensemble blending")
    
    args = p.parse_args()
    
    # GUI modu kontrol√º
    if args.gui:
        args.airnow_params = airnow_params
        return args
    
    # √ñzel durumlar i√ßin required kontrol yapma
    special_modes = args.list_cities or args.suggest_city or args.real_time or args.gui
    
    if not special_modes:
        # CLI modu i√ßin gerekli parametreleri kontrol et
        missing_args = []
        if not args.data_csv and not args.real_time:
            missing_args.append("--data_csv (veya --real-time kullanƒ±n)")
        if not args.city:
            missing_args.append("--city")
        if not args.parameter:
            missing_args.append("--parameter")
        
        if missing_args:
            # Debug modunda veya hi√ß parametre yoksa None d√∂nd√ºr (GUI i√ßin)
            import sys
            if 'debugpy' in sys.modules or len(sys.argv) == 1:
                return None  # GUI modunda a√ßmak i√ßin None d√∂nd√ºr
            else:
                # CLI modunda hata mesajƒ± g√∂ster
                print(f"‚ùå Eksik parametreler: {', '.join(missing_args)}")
                print("\nüí° Kullanƒ±m √∂rnekleri:")
                print("   python model.py --gui                    # üñ•Ô∏è GUI modunda √ßalƒ±≈ütƒ±r")
                print("   python model.py --list-cities           # üìã ≈ûehir listesi")
                print("   python model.py --suggest-city 'Los'    # üîç ≈ûehir √∂nerileri")
                print("   python model.py --real-time --city 'Denver, CO' --parameter PM25  # üåê Ger√ßek veri")
                print("   python model.py --data_csv data.csv --city 'Denver, CO' --parameter PM25  # üìÅ CSV ile")
                sys.exit(1)
    
    # airnow_params'ƒ± args'a ekle
    args.airnow_params = airnow_params
    
    return args

if __name__ == "__main__":
    try:
        args = parse_args()
        
        # parse_args None d√∂nd√ºrd√ºyse (debug/parametresiz modunda GUI a√ß)
        if args is None:
            print("üîÑ GUI modunda ba≈ülatƒ±lƒ±yor...")
            # Hi√ß y√ºklenmemi≈üse tekrar y√ºkle
            airnow_params_for_gui = import_airnow_params()
            if airnow_params_for_gui:
                gui = AQIForecastGUI(airnow_params_for_gui)
                gui.run()
            exit(0)
        
        # GUI modu
        if args.gui:
            print("üñ•Ô∏è GUI modu ba≈ülatƒ±lƒ±yor...")
            if args.airnow_params:
                print(f"‚úÖ {len(args.airnow_params.get('cities', {}))} ≈üehir y√ºklendi")
                gui = AQIForecastGUI(args.airnow_params)
                gui.run()
            else:
                print("‚ùå airnow_viewer.py entegrasyonu ba≈üarƒ±sƒ±z!")
            exit(0)
        
        # √ñzel komut kontrol√º
        if args.list_cities:
            run_pipeline(args)
            exit(0)
        
        if args.suggest_city:
            run_pipeline(args)
            exit(0)
        
        # Ana pipeline √ßalƒ±≈ütƒ±r
        print("üöÄ AeroSafe AQI Tahmin Sistemi")
        
        if args.airnow_params:
            cities_count = len(args.airnow_params.get("cities", {}))
            print(f"‚úÖ {cities_count} ≈üehir y√ºklendi")
        else:
            print("‚ö†Ô∏è airnow_viewer.py entegrasyonu ba≈üarƒ±sƒ±z")
        
        run_pipeline(args)

    except KeyboardInterrupt:
        print("\nüõë ƒ∞≈ülem kullanƒ±cƒ± tarafƒ±ndan durduruldu.")
        exit(0)
        
